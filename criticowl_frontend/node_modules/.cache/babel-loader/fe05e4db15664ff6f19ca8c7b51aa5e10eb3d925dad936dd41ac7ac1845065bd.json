{"ast":null,"code":"//     wink-nlp\n//\n//     Copyright (C) GRAYPE Systems Private Limited\n//\n//     This file is part of “wink-nlp”.\n//\n//     Permission is hereby granted, free of charge, to any\n//     person obtaining a copy of this software and\n//     associated documentation files (the \"Software\"), to\n//     deal in the Software without restriction, including\n//     without limitation the rights to use, copy, modify,\n//     merge, publish, distribute, sublicense, and/or sell\n//     copies of the Software, and to permit persons to\n//     whom the Software is furnished to do so, subject to\n//     the following conditions:\n//\n//     The above copyright notice and this permission notice\n//     shall be included in all copies or substantial\n//     portions of the Software.\n//\n//     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF\n//     ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n//     TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n//     PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n//     THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n//     DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n//     CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n//     CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n//     DEALINGS IN THE SOFTWARE.\n\n//\n\n/* eslint-disable no-console */\n/* eslint-disable no-underscore-dangle */\n\nvar recTokenizer = require('./recursive-tokenizer.js');\n\n/**\n * Creates an instance of tokenizer.\n *\n * @param  {object}   trex        language specific regular expressions needed for\n *                                tokenization. This includes helper, linear and\n *                                recursive.\n * @param  {object}   categories  tokens categories and their map to numeric code.\n * @param  {object}   preserve    language specific suffixes and prefixes to be preserved.\n * @return {function}             that performs the tokenization based on the\n *                                above configuration.\n * @private\n */\nvar tokenizer = function (trex, categories, preserve) {\n  // Maximum number of preceding spaces allowed.\n  var maxPrecedingSpaces = 65535;\n  var processFunctions = [];\n  var rgxCatDetectors = trex.ltc;\n  var tokenizeRecursively = recTokenizer(categories, preserve);\n  // Initialize helper regexes.\n  var rgxAnyWithRP = trex.helpers.anyWithRP;\n  var rgxAnyWithLP = trex.helpers.anyWithLP;\n  var rgxLPanyRP = trex.helpers.LPanyRP;\n  var rgxSplitter = trex.helpers.splitter;\n  var detectTokenCategory = function (token) {\n    // console.log( token );\n    var cat;\n    for (cat = 0; cat < rgxCatDetectors.length; cat += 1) {\n      // console.log( token, rgxCatDetectors[ cat ][ 0 ].test( token ),  rgxCatDetectors[ cat ][ 1 ] )\n      if (rgxCatDetectors[cat][0].test(token)) return rgxCatDetectors[cat][1];\n    }\n    return categories.unk;\n  }; // detectTokenCategory()\n\n  var processUnk = function (text, cat, precedingSpaces, doc, nbsp) {\n    // Match is captured here.\n    var match;\n    // Splitted non-punctuation portion's category.\n    var splitCat;\n\n    // Match with any thing followed by a **right** punctuation.\n    match = text.match(rgxAnyWithRP);\n    // Non-null indicates that there was a right punctuation in the end.\n    if (match) {\n      // Safely add the text prior to punkt if in cache.\n      splitCat = doc._addTokenIfInCache(match[1], precedingSpaces, nbsp);\n      if (splitCat === categories.unk) {\n        // Try detecting token category before falling back to recursion.\n        splitCat = detectTokenCategory(match[1]);\n        if (splitCat === categories.unk) {\n          // Still 'unk', handle it via recursive tokenizer.\n          tokenizeRecursively(trex.rtc, text, precedingSpaces, doc, nbsp);\n        } else {\n          // Because it is a detected category use `processFunctions()`.\n          processFunctions[splitCat](match[1], splitCat, precedingSpaces, doc, nbsp);\n          doc._addToken(match[2], categories.punctuation, 0, nbsp);\n        }\n      } else {\n        // The split is a added via `addTokenIfInCache()`, simply add the balance.\n        doc._addToken(match[2], categories.punctuation, 0, nbsp);\n      }\n      // All done so,\n      return;\n    }\n    // Match with any thing followed by a **left** punctuation.\n    match = text.match(rgxAnyWithLP);\n    // Now non-null indicates that there was a left punctuation in the beginning.\n    if (match) {\n      // If match 2 is a valid lexeme, can safley add tokens. Notice insertion\n      // sequence has reversed compared to the previous if block.\n      if (doc.isLexeme(match[2])) {\n        doc._addToken(match[1], categories.punctuation, precedingSpaces, nbsp);\n        doc._addTokenIfInCache(match[2], 0, nbsp);\n      } else {\n        // Try detecting token category before falling bac k to recursion.\n        splitCat = detectTokenCategory(match[2]);\n        if (splitCat === categories.unk) {\n          // Still 'unk', handle it via recursive tokenizer.\n          tokenizeRecursively(trex.rtc, text, precedingSpaces, doc, nbsp);\n        } else {\n          // Because it is a detected category use `processFunctions()`.\n          doc._addToken(match[1], categories.punctuation, precedingSpaces, nbsp);\n          processFunctions[splitCat](match[2], splitCat, 0, doc, nbsp);\n        }\n      }\n      // All done so,\n      return;\n    }\n    // Punctuation on both sides!\n    match = text.match(rgxLPanyRP);\n    if (match) {\n      // If match 2 is a valid lexeme, can safley add tokens.\n      if (doc.isLexeme(match[2])) {\n        doc._addToken(match[1], categories.punctuation, precedingSpaces, nbsp);\n        doc._addTokenIfInCache(match[2], 0, nbsp);\n        doc._addToken(match[3], categories.punctuation, 0, nbsp);\n      } else {\n        // Try detecting token category before falling bac k to recursion.\n        splitCat = detectTokenCategory(match[2]);\n        if (splitCat === categories.unk) {\n          // Still 'unk', handle it via recursive tokenizer.\n          tokenizeRecursively(trex.rtc, text, precedingSpaces, doc, nbsp);\n        } else {\n          // Because it is a detected category use `processFunctions()`.\n          doc._addToken(match[1], categories.punctuation, precedingSpaces, nbsp);\n          processFunctions[splitCat](match[2], splitCat, 0, doc, nbsp);\n          doc._addToken(match[3], categories.punctuation, 0, nbsp);\n        }\n      }\n      // All done so,\n      return;\n    }\n\n    // Nothing worked, treat the whole thing as `unk` and fallback to recursive tokenizer.\n    tokenizeRecursively(trex.rtc, text, precedingSpaces, doc, nbsp);\n  }; // processUnk()\n\n  // var processWord = function ( token, cat, precedingSpaces, doc ) {\n  //   doc._addToken( token, cat, precedingSpaces );\n  // }; // processWord()\n\n  var processWordRP = function (token, cat, precedingSpaces, doc, nbsp) {\n    // Handle **special case**, `^[a-z]\\.$` will arrive here instead of `shortForm`!\n    var tl = token.length;\n    if (tl > 2) {\n      doc._addToken(token.slice(0, -1), categories.word, precedingSpaces, nbsp);\n      doc._addToken(token.slice(-1), categories.punctuation, 0, nbsp);\n    } else if (tl === 2 && token[tl - 1] === '.') {\n      doc._addToken(token, categories.word, precedingSpaces, nbsp);\n    } else {\n      doc._addToken(token.slice(0, -1), categories.word, precedingSpaces, nbsp);\n      doc._addToken(token.slice(-1), categories.punctuation, 0, nbsp);\n    }\n  }; // processWordRP()\n\n  var processDefault = function (token, cat, precedingSpaces, doc, nbsp) {\n    doc._addToken(token, cat, precedingSpaces, nbsp);\n  }; // processDefault()\n\n  var tokenize = function (doc, text) {\n    // Raw tokens, obtained by splitting them on spaces.\n    var rawTokens = [];\n    // Contains the number of spaces preceding a token.\n    var precedingSpaces = 0;\n    // Non breaking spaces.\n    var nbSpaces = null;\n    // Pointer to the `rawTokens`, whereas `pp` is the previous pointer!\n    var p;\n    // Token category as detected by the `detectTokenCategory()` function.\n    var cat;\n    // A temporary token!\n    var t;\n    rawTokens = text.split(rgxSplitter);\n\n    // Now process each raw token.\n    for (p = 0; p < rawTokens.length; p += 1) {\n      t = rawTokens[p];\n      // Skip empty (`''`) token.\n      if (!t) continue; // eslint-disable-line no-continue\n      // Non-empty token:\n      const hasNBSP = /[\\u00a0\\u2002-\\u2005\\u2009\\u200a\\u202f\\u205f]/.test(t);\n      if (t[0] === ' ' || hasNBSP) {\n        // This indicates spaces: count them.\n        precedingSpaces = t.length;\n        if (hasNBSP) {\n          nbSpaces = t;\n          precedingSpaces = maxPrecedingSpaces;\n        } else if (precedingSpaces > maxPrecedingSpaces - 1) precedingSpaces = maxPrecedingSpaces - 1;\n        // Cap precedingSpaces to a limit if it exceeds it.\n        // if ( precedingSpaces > maxPrecedingSpaces - 1 ) precedingSpaces = maxPrecedingSpaces - 1;\n      } else {\n        // A potential token: process it.\n        cat = doc._addTokenIfInCache(t, precedingSpaces, nbSpaces);\n        if (cat === categories.unk) {\n          cat = detectTokenCategory(t);\n          processFunctions[cat](t, cat, precedingSpaces, doc, nbSpaces);\n        }\n        precedingSpaces = 0;\n        nbSpaces = null;\n      }\n    } // for\n  }; // tokenize()\n\n  // Main Code:\n  // Specific Processes.\n  processFunctions[categories.unk] = processUnk;\n  processFunctions[categories.wordRP] = processWordRP;\n\n  // Default process.\n  processFunctions[categories.emoji] = processDefault;\n  processFunctions[categories.word] = processDefault;\n  processFunctions[categories.shortForm] = processDefault;\n  processFunctions[categories.number] = processDefault;\n  processFunctions[categories.url] = processDefault;\n  processFunctions[categories.email] = processDefault;\n  processFunctions[categories.mention] = processDefault;\n  processFunctions[categories.hashtag] = processDefault;\n  processFunctions[categories.emoticon] = processDefault;\n  processFunctions[categories.time] = processDefault;\n  processFunctions[categories.ordinal] = processDefault;\n  processFunctions[categories.currency] = processDefault;\n  processFunctions[categories.punctuation] = processDefault;\n  processFunctions[categories.symbol] = processDefault;\n  processFunctions[categories.tabCRLF] = processDefault;\n  processFunctions[categories.apos] = processDefault;\n  processFunctions[categories.alpha] = processDefault;\n  processFunctions[categories.decade] = processDefault;\n  return tokenize;\n}; // tokenizer()\n\nmodule.exports = tokenizer;","map":{"version":3,"names":["recTokenizer","require","tokenizer","trex","categories","preserve","maxPrecedingSpaces","processFunctions","rgxCatDetectors","ltc","tokenizeRecursively","rgxAnyWithRP","helpers","anyWithRP","rgxAnyWithLP","anyWithLP","rgxLPanyRP","LPanyRP","rgxSplitter","splitter","detectTokenCategory","token","cat","length","test","unk","processUnk","text","precedingSpaces","doc","nbsp","match","splitCat","_addTokenIfInCache","rtc","_addToken","punctuation","isLexeme","processWordRP","tl","slice","word","processDefault","tokenize","rawTokens","nbSpaces","p","t","split","hasNBSP","wordRP","emoji","shortForm","number","url","email","mention","hashtag","emoticon","time","ordinal","currency","symbol","tabCRLF","apos","alpha","decade","module","exports"],"sources":["C:/Users/cheko/Desktop/Education/Freelance/criticowl-main/criticowl_frontend/node_modules/wink-nlp/src/tokenizer.js"],"sourcesContent":["//     wink-nlp\n//\n//     Copyright (C) GRAYPE Systems Private Limited\n//\n//     This file is part of “wink-nlp”.\n//\n//     Permission is hereby granted, free of charge, to any\n//     person obtaining a copy of this software and\n//     associated documentation files (the \"Software\"), to\n//     deal in the Software without restriction, including\n//     without limitation the rights to use, copy, modify,\n//     merge, publish, distribute, sublicense, and/or sell\n//     copies of the Software, and to permit persons to\n//     whom the Software is furnished to do so, subject to\n//     the following conditions:\n//\n//     The above copyright notice and this permission notice\n//     shall be included in all copies or substantial\n//     portions of the Software.\n//\n//     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF\n//     ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n//     TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n//     PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n//     THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n//     DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n//     CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n//     CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n//     DEALINGS IN THE SOFTWARE.\n\n//\n\n/* eslint-disable no-console */\n/* eslint-disable no-underscore-dangle */\n\nvar recTokenizer = require( './recursive-tokenizer.js' );\n\n/**\n * Creates an instance of tokenizer.\n *\n * @param  {object}   trex        language specific regular expressions needed for\n *                                tokenization. This includes helper, linear and\n *                                recursive.\n * @param  {object}   categories  tokens categories and their map to numeric code.\n * @param  {object}   preserve    language specific suffixes and prefixes to be preserved.\n * @return {function}             that performs the tokenization based on the\n *                                above configuration.\n * @private\n */\nvar tokenizer = function ( trex, categories, preserve ) {\n  // Maximum number of preceding spaces allowed.\n  var maxPrecedingSpaces = 65535;\n  var processFunctions = [];\n  var rgxCatDetectors = trex.ltc;\n  var tokenizeRecursively = recTokenizer( categories, preserve );\n  // Initialize helper regexes.\n  var rgxAnyWithRP = trex.helpers.anyWithRP;\n  var rgxAnyWithLP = trex.helpers.anyWithLP;\n  var rgxLPanyRP = trex.helpers.LPanyRP;\n  var rgxSplitter = trex.helpers.splitter;\n\n  var detectTokenCategory = function ( token ) {\n    // console.log( token );\n    var cat;\n    for ( cat = 0; cat < rgxCatDetectors.length; cat += 1 ) {\n      // console.log( token, rgxCatDetectors[ cat ][ 0 ].test( token ),  rgxCatDetectors[ cat ][ 1 ] )\n      if ( rgxCatDetectors[ cat ][ 0 ].test( token ) ) return rgxCatDetectors[ cat ][ 1 ];\n    }\n    return categories.unk;\n  }; // detectTokenCategory()\n\n\n  var processUnk = function ( text, cat, precedingSpaces, doc, nbsp ) {\n    // Match is captured here.\n    var match;\n    // Splitted non-punctuation portion's category.\n    var splitCat;\n\n    // Match with any thing followed by a **right** punctuation.\n    match = text.match( rgxAnyWithRP );\n    // Non-null indicates that there was a right punctuation in the end.\n    if ( match ) {\n      // Safely add the text prior to punkt if in cache.\n      splitCat = doc._addTokenIfInCache( match[ 1 ], precedingSpaces, nbsp );\n      if ( splitCat === categories.unk ) {\n        // Try detecting token category before falling back to recursion.\n        splitCat = detectTokenCategory( match[ 1 ] );\n        if ( splitCat  === categories.unk ) {\n          // Still 'unk', handle it via recursive tokenizer.\n          tokenizeRecursively( trex.rtc, text, precedingSpaces, doc, nbsp );\n        } else {\n          // Because it is a detected category use `processFunctions()`.\n          processFunctions[ splitCat ]( match[ 1 ], splitCat, precedingSpaces, doc, nbsp );\n          doc._addToken( match[ 2 ], categories.punctuation, 0, nbsp );\n        }\n      } else {\n        // The split is a added via `addTokenIfInCache()`, simply add the balance.\n        doc._addToken( match[ 2 ], categories.punctuation, 0, nbsp );\n      }\n      // All done so,\n      return;\n    }\n    // Match with any thing followed by a **left** punctuation.\n    match = text.match( rgxAnyWithLP );\n    // Now non-null indicates that there was a left punctuation in the beginning.\n    if ( match ) {\n      // If match 2 is a valid lexeme, can safley add tokens. Notice insertion\n      // sequence has reversed compared to the previous if block.\n      if ( doc.isLexeme( match[ 2 ] ) ) {\n        doc._addToken( match[ 1 ], categories.punctuation, precedingSpaces, nbsp );\n        doc._addTokenIfInCache( match[ 2 ], 0, nbsp );\n      } else {\n        // Try detecting token category before falling bac k to recursion.\n        splitCat = detectTokenCategory( match[ 2 ] );\n        if ( splitCat  === categories.unk ) {\n          // Still 'unk', handle it via recursive tokenizer.\n          tokenizeRecursively( trex.rtc, text, precedingSpaces, doc, nbsp );\n        } else {\n          // Because it is a detected category use `processFunctions()`.\n          doc._addToken( match[ 1 ], categories.punctuation, precedingSpaces, nbsp );\n          processFunctions[ splitCat ]( match[ 2 ], splitCat, 0, doc, nbsp );\n        }\n      }\n      // All done so,\n      return;\n    }\n    // Punctuation on both sides!\n    match = text.match( rgxLPanyRP );\n    if ( match ) {\n      // If match 2 is a valid lexeme, can safley add tokens.\n      if ( doc.isLexeme( match[ 2 ] ) ) {\n        doc._addToken( match[ 1 ], categories.punctuation, precedingSpaces, nbsp );\n        doc._addTokenIfInCache( match[ 2 ], 0, nbsp );\n        doc._addToken( match[ 3 ], categories.punctuation, 0, nbsp );\n      } else {\n        // Try detecting token category before falling bac k to recursion.\n        splitCat = detectTokenCategory( match[ 2 ] );\n        if ( splitCat  === categories.unk ) {\n          // Still 'unk', handle it via recursive tokenizer.\n          tokenizeRecursively( trex.rtc, text, precedingSpaces, doc, nbsp );\n        } else {\n          // Because it is a detected category use `processFunctions()`.\n          doc._addToken( match[ 1 ], categories.punctuation, precedingSpaces, nbsp );\n          processFunctions[ splitCat ]( match[ 2 ], splitCat, 0, doc, nbsp );\n          doc._addToken( match[ 3 ], categories.punctuation, 0, nbsp );\n        }\n      }\n      // All done so,\n      return;\n    }\n\n    // Nothing worked, treat the whole thing as `unk` and fallback to recursive tokenizer.\n    tokenizeRecursively( trex.rtc, text, precedingSpaces, doc, nbsp );\n  }; // processUnk()\n\n  // var processWord = function ( token, cat, precedingSpaces, doc ) {\n  //   doc._addToken( token, cat, precedingSpaces );\n  // }; // processWord()\n\n  var processWordRP = function ( token, cat, precedingSpaces, doc, nbsp ) {\n    // Handle **special case**, `^[a-z]\\.$` will arrive here instead of `shortForm`!\n    var tl = token.length;\n    if ( tl > 2 ) {\n      doc._addToken( token.slice( 0, -1 ), categories.word, precedingSpaces, nbsp );\n      doc._addToken( token.slice( -1 ), categories.punctuation, 0, nbsp );\n    } else if ( tl === 2 && token[ tl - 1 ] === '.' ) {\n        doc._addToken( token, categories.word, precedingSpaces, nbsp );\n      } else {\n        doc._addToken( token.slice( 0, -1 ), categories.word, precedingSpaces, nbsp );\n        doc._addToken( token.slice( -1 ), categories.punctuation, 0, nbsp );\n      }\n  }; // processWordRP()\n\n  var processDefault = function ( token, cat, precedingSpaces, doc, nbsp ) {\n    doc._addToken( token, cat, precedingSpaces, nbsp );\n  }; // processDefault()\n\n  var tokenize = function ( doc, text ) {\n    // Raw tokens, obtained by splitting them on spaces.\n    var rawTokens = [];\n    // Contains the number of spaces preceding a token.\n    var precedingSpaces = 0;\n    // Non breaking spaces.\n    var nbSpaces = null;\n    // Pointer to the `rawTokens`, whereas `pp` is the previous pointer!\n    var p;\n    // Token category as detected by the `detectTokenCategory()` function.\n    var cat;\n    // A temporary token!\n    var t;\n\n    rawTokens = text.split( rgxSplitter );\n\n    // Now process each raw token.\n    for ( p = 0; p < rawTokens.length; p += 1 ) {\n      t = rawTokens[ p ];\n      // Skip empty (`''`) token.\n      if ( !t ) continue; // eslint-disable-line no-continue\n      // Non-empty token:\n      const hasNBSP = ( /[\\u00a0\\u2002-\\u2005\\u2009\\u200a\\u202f\\u205f]/ ).test( t );\n      if ( t[ 0 ] === ' ' || hasNBSP ) {\n        // This indicates spaces: count them.\n        precedingSpaces = t.length;\n        if ( hasNBSP ) {\n          nbSpaces = t;\n          precedingSpaces = maxPrecedingSpaces;\n        } else if ( precedingSpaces > maxPrecedingSpaces - 1 ) precedingSpaces = maxPrecedingSpaces - 1;\n        // Cap precedingSpaces to a limit if it exceeds it.\n        // if ( precedingSpaces > maxPrecedingSpaces - 1 ) precedingSpaces = maxPrecedingSpaces - 1;\n      } else {\n        // A potential token: process it.\n        cat = doc._addTokenIfInCache( t, precedingSpaces, nbSpaces );\n        if ( cat === categories.unk ) {\n          cat = detectTokenCategory( t );\n          processFunctions[ cat ]( t, cat, precedingSpaces, doc, nbSpaces );\n        }\n        precedingSpaces = 0;\n        nbSpaces = null;\n      }\n    } // for\n  }; // tokenize()\n\n  // Main Code:\n  // Specific Processes.\n  processFunctions[ categories.unk ] = processUnk;\n  processFunctions[ categories.wordRP ] = processWordRP;\n\n  // Default process.\n  processFunctions[ categories.emoji ] = processDefault;\n  processFunctions[ categories.word ] = processDefault;\n  processFunctions[ categories.shortForm ] = processDefault;\n  processFunctions[ categories.number ] = processDefault;\n  processFunctions[ categories.url ] = processDefault;\n  processFunctions[ categories.email ] = processDefault;\n  processFunctions[ categories.mention ] = processDefault;\n  processFunctions[ categories.hashtag ] = processDefault;\n  processFunctions[ categories.emoticon ] = processDefault;\n  processFunctions[ categories.time ] = processDefault;\n  processFunctions[ categories.ordinal ] = processDefault;\n  processFunctions[ categories.currency ] = processDefault;\n  processFunctions[ categories.punctuation ] = processDefault;\n  processFunctions[ categories.symbol ] = processDefault;\n  processFunctions[ categories.tabCRLF ] = processDefault;\n  processFunctions[ categories.apos ] = processDefault;\n  processFunctions[ categories.alpha ] = processDefault;\n  processFunctions[ categories.decade ] = processDefault;\n\n  return tokenize;\n}; // tokenizer()\n\nmodule.exports = tokenizer;\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,IAAIA,YAAY,GAAGC,OAAO,CAAE,0BAA2B,CAAC;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAIC,SAAS,GAAG,SAAAA,CAAWC,IAAI,EAAEC,UAAU,EAAEC,QAAQ,EAAG;EACtD;EACA,IAAIC,kBAAkB,GAAG,KAAK;EAC9B,IAAIC,gBAAgB,GAAG,EAAE;EACzB,IAAIC,eAAe,GAAGL,IAAI,CAACM,GAAG;EAC9B,IAAIC,mBAAmB,GAAGV,YAAY,CAAEI,UAAU,EAAEC,QAAS,CAAC;EAC9D;EACA,IAAIM,YAAY,GAAGR,IAAI,CAACS,OAAO,CAACC,SAAS;EACzC,IAAIC,YAAY,GAAGX,IAAI,CAACS,OAAO,CAACG,SAAS;EACzC,IAAIC,UAAU,GAAGb,IAAI,CAACS,OAAO,CAACK,OAAO;EACrC,IAAIC,WAAW,GAAGf,IAAI,CAACS,OAAO,CAACO,QAAQ;EAEvC,IAAIC,mBAAmB,GAAG,SAAAA,CAAWC,KAAK,EAAG;IAC3C;IACA,IAAIC,GAAG;IACP,KAAMA,GAAG,GAAG,CAAC,EAAEA,GAAG,GAAGd,eAAe,CAACe,MAAM,EAAED,GAAG,IAAI,CAAC,EAAG;MACtD;MACA,IAAKd,eAAe,CAAEc,GAAG,CAAE,CAAE,CAAC,CAAE,CAACE,IAAI,CAAEH,KAAM,CAAC,EAAG,OAAOb,eAAe,CAAEc,GAAG,CAAE,CAAE,CAAC,CAAE;IACrF;IACA,OAAOlB,UAAU,CAACqB,GAAG;EACvB,CAAC,CAAC,CAAC;;EAGH,IAAIC,UAAU,GAAG,SAAAA,CAAWC,IAAI,EAAEL,GAAG,EAAEM,eAAe,EAAEC,GAAG,EAAEC,IAAI,EAAG;IAClE;IACA,IAAIC,KAAK;IACT;IACA,IAAIC,QAAQ;;IAEZ;IACAD,KAAK,GAAGJ,IAAI,CAACI,KAAK,CAAEpB,YAAa,CAAC;IAClC;IACA,IAAKoB,KAAK,EAAG;MACX;MACAC,QAAQ,GAAGH,GAAG,CAACI,kBAAkB,CAAEF,KAAK,CAAE,CAAC,CAAE,EAAEH,eAAe,EAAEE,IAAK,CAAC;MACtE,IAAKE,QAAQ,KAAK5B,UAAU,CAACqB,GAAG,EAAG;QACjC;QACAO,QAAQ,GAAGZ,mBAAmB,CAAEW,KAAK,CAAE,CAAC,CAAG,CAAC;QAC5C,IAAKC,QAAQ,KAAM5B,UAAU,CAACqB,GAAG,EAAG;UAClC;UACAf,mBAAmB,CAAEP,IAAI,CAAC+B,GAAG,EAAEP,IAAI,EAAEC,eAAe,EAAEC,GAAG,EAAEC,IAAK,CAAC;QACnE,CAAC,MAAM;UACL;UACAvB,gBAAgB,CAAEyB,QAAQ,CAAE,CAAED,KAAK,CAAE,CAAC,CAAE,EAAEC,QAAQ,EAAEJ,eAAe,EAAEC,GAAG,EAAEC,IAAK,CAAC;UAChFD,GAAG,CAACM,SAAS,CAAEJ,KAAK,CAAE,CAAC,CAAE,EAAE3B,UAAU,CAACgC,WAAW,EAAE,CAAC,EAAEN,IAAK,CAAC;QAC9D;MACF,CAAC,MAAM;QACL;QACAD,GAAG,CAACM,SAAS,CAAEJ,KAAK,CAAE,CAAC,CAAE,EAAE3B,UAAU,CAACgC,WAAW,EAAE,CAAC,EAAEN,IAAK,CAAC;MAC9D;MACA;MACA;IACF;IACA;IACAC,KAAK,GAAGJ,IAAI,CAACI,KAAK,CAAEjB,YAAa,CAAC;IAClC;IACA,IAAKiB,KAAK,EAAG;MACX;MACA;MACA,IAAKF,GAAG,CAACQ,QAAQ,CAAEN,KAAK,CAAE,CAAC,CAAG,CAAC,EAAG;QAChCF,GAAG,CAACM,SAAS,CAAEJ,KAAK,CAAE,CAAC,CAAE,EAAE3B,UAAU,CAACgC,WAAW,EAAER,eAAe,EAAEE,IAAK,CAAC;QAC1ED,GAAG,CAACI,kBAAkB,CAAEF,KAAK,CAAE,CAAC,CAAE,EAAE,CAAC,EAAED,IAAK,CAAC;MAC/C,CAAC,MAAM;QACL;QACAE,QAAQ,GAAGZ,mBAAmB,CAAEW,KAAK,CAAE,CAAC,CAAG,CAAC;QAC5C,IAAKC,QAAQ,KAAM5B,UAAU,CAACqB,GAAG,EAAG;UAClC;UACAf,mBAAmB,CAAEP,IAAI,CAAC+B,GAAG,EAAEP,IAAI,EAAEC,eAAe,EAAEC,GAAG,EAAEC,IAAK,CAAC;QACnE,CAAC,MAAM;UACL;UACAD,GAAG,CAACM,SAAS,CAAEJ,KAAK,CAAE,CAAC,CAAE,EAAE3B,UAAU,CAACgC,WAAW,EAAER,eAAe,EAAEE,IAAK,CAAC;UAC1EvB,gBAAgB,CAAEyB,QAAQ,CAAE,CAAED,KAAK,CAAE,CAAC,CAAE,EAAEC,QAAQ,EAAE,CAAC,EAAEH,GAAG,EAAEC,IAAK,CAAC;QACpE;MACF;MACA;MACA;IACF;IACA;IACAC,KAAK,GAAGJ,IAAI,CAACI,KAAK,CAAEf,UAAW,CAAC;IAChC,IAAKe,KAAK,EAAG;MACX;MACA,IAAKF,GAAG,CAACQ,QAAQ,CAAEN,KAAK,CAAE,CAAC,CAAG,CAAC,EAAG;QAChCF,GAAG,CAACM,SAAS,CAAEJ,KAAK,CAAE,CAAC,CAAE,EAAE3B,UAAU,CAACgC,WAAW,EAAER,eAAe,EAAEE,IAAK,CAAC;QAC1ED,GAAG,CAACI,kBAAkB,CAAEF,KAAK,CAAE,CAAC,CAAE,EAAE,CAAC,EAAED,IAAK,CAAC;QAC7CD,GAAG,CAACM,SAAS,CAAEJ,KAAK,CAAE,CAAC,CAAE,EAAE3B,UAAU,CAACgC,WAAW,EAAE,CAAC,EAAEN,IAAK,CAAC;MAC9D,CAAC,MAAM;QACL;QACAE,QAAQ,GAAGZ,mBAAmB,CAAEW,KAAK,CAAE,CAAC,CAAG,CAAC;QAC5C,IAAKC,QAAQ,KAAM5B,UAAU,CAACqB,GAAG,EAAG;UAClC;UACAf,mBAAmB,CAAEP,IAAI,CAAC+B,GAAG,EAAEP,IAAI,EAAEC,eAAe,EAAEC,GAAG,EAAEC,IAAK,CAAC;QACnE,CAAC,MAAM;UACL;UACAD,GAAG,CAACM,SAAS,CAAEJ,KAAK,CAAE,CAAC,CAAE,EAAE3B,UAAU,CAACgC,WAAW,EAAER,eAAe,EAAEE,IAAK,CAAC;UAC1EvB,gBAAgB,CAAEyB,QAAQ,CAAE,CAAED,KAAK,CAAE,CAAC,CAAE,EAAEC,QAAQ,EAAE,CAAC,EAAEH,GAAG,EAAEC,IAAK,CAAC;UAClED,GAAG,CAACM,SAAS,CAAEJ,KAAK,CAAE,CAAC,CAAE,EAAE3B,UAAU,CAACgC,WAAW,EAAE,CAAC,EAAEN,IAAK,CAAC;QAC9D;MACF;MACA;MACA;IACF;;IAEA;IACApB,mBAAmB,CAAEP,IAAI,CAAC+B,GAAG,EAAEP,IAAI,EAAEC,eAAe,EAAEC,GAAG,EAAEC,IAAK,CAAC;EACnE,CAAC,CAAC,CAAC;;EAEH;EACA;EACA;;EAEA,IAAIQ,aAAa,GAAG,SAAAA,CAAWjB,KAAK,EAAEC,GAAG,EAAEM,eAAe,EAAEC,GAAG,EAAEC,IAAI,EAAG;IACtE;IACA,IAAIS,EAAE,GAAGlB,KAAK,CAACE,MAAM;IACrB,IAAKgB,EAAE,GAAG,CAAC,EAAG;MACZV,GAAG,CAACM,SAAS,CAAEd,KAAK,CAACmB,KAAK,CAAE,CAAC,EAAE,CAAC,CAAE,CAAC,EAAEpC,UAAU,CAACqC,IAAI,EAAEb,eAAe,EAAEE,IAAK,CAAC;MAC7ED,GAAG,CAACM,SAAS,CAAEd,KAAK,CAACmB,KAAK,CAAE,CAAC,CAAE,CAAC,EAAEpC,UAAU,CAACgC,WAAW,EAAE,CAAC,EAAEN,IAAK,CAAC;IACrE,CAAC,MAAM,IAAKS,EAAE,KAAK,CAAC,IAAIlB,KAAK,CAAEkB,EAAE,GAAG,CAAC,CAAE,KAAK,GAAG,EAAG;MAC9CV,GAAG,CAACM,SAAS,CAAEd,KAAK,EAAEjB,UAAU,CAACqC,IAAI,EAAEb,eAAe,EAAEE,IAAK,CAAC;IAChE,CAAC,MAAM;MACLD,GAAG,CAACM,SAAS,CAAEd,KAAK,CAACmB,KAAK,CAAE,CAAC,EAAE,CAAC,CAAE,CAAC,EAAEpC,UAAU,CAACqC,IAAI,EAAEb,eAAe,EAAEE,IAAK,CAAC;MAC7ED,GAAG,CAACM,SAAS,CAAEd,KAAK,CAACmB,KAAK,CAAE,CAAC,CAAE,CAAC,EAAEpC,UAAU,CAACgC,WAAW,EAAE,CAAC,EAAEN,IAAK,CAAC;IACrE;EACJ,CAAC,CAAC,CAAC;;EAEH,IAAIY,cAAc,GAAG,SAAAA,CAAWrB,KAAK,EAAEC,GAAG,EAAEM,eAAe,EAAEC,GAAG,EAAEC,IAAI,EAAG;IACvED,GAAG,CAACM,SAAS,CAAEd,KAAK,EAAEC,GAAG,EAAEM,eAAe,EAAEE,IAAK,CAAC;EACpD,CAAC,CAAC,CAAC;;EAEH,IAAIa,QAAQ,GAAG,SAAAA,CAAWd,GAAG,EAAEF,IAAI,EAAG;IACpC;IACA,IAAIiB,SAAS,GAAG,EAAE;IAClB;IACA,IAAIhB,eAAe,GAAG,CAAC;IACvB;IACA,IAAIiB,QAAQ,GAAG,IAAI;IACnB;IACA,IAAIC,CAAC;IACL;IACA,IAAIxB,GAAG;IACP;IACA,IAAIyB,CAAC;IAELH,SAAS,GAAGjB,IAAI,CAACqB,KAAK,CAAE9B,WAAY,CAAC;;IAErC;IACA,KAAM4B,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGF,SAAS,CAACrB,MAAM,EAAEuB,CAAC,IAAI,CAAC,EAAG;MAC1CC,CAAC,GAAGH,SAAS,CAAEE,CAAC,CAAE;MAClB;MACA,IAAK,CAACC,CAAC,EAAG,SAAS,CAAC;MACpB;MACA,MAAME,OAAO,GAAK,+CAA+C,CAAGzB,IAAI,CAAEuB,CAAE,CAAC;MAC7E,IAAKA,CAAC,CAAE,CAAC,CAAE,KAAK,GAAG,IAAIE,OAAO,EAAG;QAC/B;QACArB,eAAe,GAAGmB,CAAC,CAACxB,MAAM;QAC1B,IAAK0B,OAAO,EAAG;UACbJ,QAAQ,GAAGE,CAAC;UACZnB,eAAe,GAAGtB,kBAAkB;QACtC,CAAC,MAAM,IAAKsB,eAAe,GAAGtB,kBAAkB,GAAG,CAAC,EAAGsB,eAAe,GAAGtB,kBAAkB,GAAG,CAAC;QAC/F;QACA;MACF,CAAC,MAAM;QACL;QACAgB,GAAG,GAAGO,GAAG,CAACI,kBAAkB,CAAEc,CAAC,EAAEnB,eAAe,EAAEiB,QAAS,CAAC;QAC5D,IAAKvB,GAAG,KAAKlB,UAAU,CAACqB,GAAG,EAAG;UAC5BH,GAAG,GAAGF,mBAAmB,CAAE2B,CAAE,CAAC;UAC9BxC,gBAAgB,CAAEe,GAAG,CAAE,CAAEyB,CAAC,EAAEzB,GAAG,EAAEM,eAAe,EAAEC,GAAG,EAAEgB,QAAS,CAAC;QACnE;QACAjB,eAAe,GAAG,CAAC;QACnBiB,QAAQ,GAAG,IAAI;MACjB;IACF,CAAC,CAAC;EACJ,CAAC,CAAC,CAAC;;EAEH;EACA;EACAtC,gBAAgB,CAAEH,UAAU,CAACqB,GAAG,CAAE,GAAGC,UAAU;EAC/CnB,gBAAgB,CAAEH,UAAU,CAAC8C,MAAM,CAAE,GAAGZ,aAAa;;EAErD;EACA/B,gBAAgB,CAAEH,UAAU,CAAC+C,KAAK,CAAE,GAAGT,cAAc;EACrDnC,gBAAgB,CAAEH,UAAU,CAACqC,IAAI,CAAE,GAAGC,cAAc;EACpDnC,gBAAgB,CAAEH,UAAU,CAACgD,SAAS,CAAE,GAAGV,cAAc;EACzDnC,gBAAgB,CAAEH,UAAU,CAACiD,MAAM,CAAE,GAAGX,cAAc;EACtDnC,gBAAgB,CAAEH,UAAU,CAACkD,GAAG,CAAE,GAAGZ,cAAc;EACnDnC,gBAAgB,CAAEH,UAAU,CAACmD,KAAK,CAAE,GAAGb,cAAc;EACrDnC,gBAAgB,CAAEH,UAAU,CAACoD,OAAO,CAAE,GAAGd,cAAc;EACvDnC,gBAAgB,CAAEH,UAAU,CAACqD,OAAO,CAAE,GAAGf,cAAc;EACvDnC,gBAAgB,CAAEH,UAAU,CAACsD,QAAQ,CAAE,GAAGhB,cAAc;EACxDnC,gBAAgB,CAAEH,UAAU,CAACuD,IAAI,CAAE,GAAGjB,cAAc;EACpDnC,gBAAgB,CAAEH,UAAU,CAACwD,OAAO,CAAE,GAAGlB,cAAc;EACvDnC,gBAAgB,CAAEH,UAAU,CAACyD,QAAQ,CAAE,GAAGnB,cAAc;EACxDnC,gBAAgB,CAAEH,UAAU,CAACgC,WAAW,CAAE,GAAGM,cAAc;EAC3DnC,gBAAgB,CAAEH,UAAU,CAAC0D,MAAM,CAAE,GAAGpB,cAAc;EACtDnC,gBAAgB,CAAEH,UAAU,CAAC2D,OAAO,CAAE,GAAGrB,cAAc;EACvDnC,gBAAgB,CAAEH,UAAU,CAAC4D,IAAI,CAAE,GAAGtB,cAAc;EACpDnC,gBAAgB,CAAEH,UAAU,CAAC6D,KAAK,CAAE,GAAGvB,cAAc;EACrDnC,gBAAgB,CAAEH,UAAU,CAAC8D,MAAM,CAAE,GAAGxB,cAAc;EAEtD,OAAOC,QAAQ;AACjB,CAAC,CAAC,CAAC;;AAEHwB,MAAM,CAACC,OAAO,GAAGlE,SAAS","ignoreList":[]},"metadata":{},"sourceType":"script","externalDependencies":[]}