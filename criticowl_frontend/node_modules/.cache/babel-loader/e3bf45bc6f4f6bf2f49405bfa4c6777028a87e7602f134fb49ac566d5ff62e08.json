{"ast":null,"code":"//     wink-nlp\n//\n//     Copyright (C) GRAYPE Systems Private Limited\n//\n//     This file is part of “wink-nlp”.\n//\n//     Permission is hereby granted, free of charge, to any\n//     person obtaining a copy of this software and\n//     associated documentation files (the \"Software\"), to\n//     deal in the Software without restriction, including\n//     without limitation the rights to use, copy, modify,\n//     merge, publish, distribute, sublicense, and/or sell\n//     copies of the Software, and to permit persons to\n//     whom the Software is furnished to do so, subject to\n//     the following conditions:\n//\n//     The above copyright notice and this permission notice\n//     shall be included in all copies or substantial\n//     portions of the Software.\n//\n//     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF\n//     ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n//     TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n//     PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n//     THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n//     DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n//     CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n//     CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n//     DEALINGS IN THE SOFTWARE.\n\n//\n\nvar DocDataWrapper = require('./dd-wrapper.js');\nvar Doc = require('./doc-v2.js');\nvar Cache = require('./cache.js');\nvar tokenizer = require('./tokenizer.js');\nvar compileTRex = require('./compile-trex.js');\nvar mappers = require('./tokens-mappers.js');\nvar itsHelpers = require('./its.js');\nvar asHelpers = require('./as.js');\nvar mapRawTokens2UIdOfNormal = mappers.mapRawTokens2UIdOfNormal;\nvar mapRawTokens2UIdOfDefaultPOS = mappers.mapRawTokens2UIdOfDefaultPOS;\nvar Compiler = require('./examples-compiler.js');\nvar constants = require('./constants.js');\nvar fsm = require('./automaton.js');\nvar search = require('./search.js');\nvar locate = require('./locate.js');\nvar helper = require('./helper.js');\n\n// Size of a single token.\nvar tkSize = constants.tkSize;\n\n/**\n * Creates an instance of nlp.\n * @private\n *\n * @param {object} theModel language model.\n * @param {string[]} pipe of nlp annotations.\n * @param {object} wordEmbeddings object read using node require.\n * @returns {object} conatining set of API methods for natural language processing.\n * @example\n * const nlp = require( 'wink-nlp' );\n * var myNLP = nlp();\n*/\nvar nlp = function (theModel, pipe = null, wordEmbeddings = null) {\n  var methods = Object.create(null);\n  // Token Regex; compiled from `model`\n  var trex;\n  // wink-nlp language `model`.\n  var model;\n  // Holds instance of `cache` created using the `model`.\n  var cache;\n  // NLP Pipe Config.\n  // var nlpPipe = Object.create( null );\n  // Configured tokenize.\n  var tokenize;\n  // Automata\n  // 1. NER\n  var nerAutomata;\n  var nerTransformers;\n  // 2. SBD\n  var sbdAutomata;\n  var sbdTransformers;\n  var sbdSetter;\n  // 3. NEG\n  var negAutomata;\n  var negSetter;\n  // SA\n  var saAutomata;\n  var saSetter;\n  // POS\n  var posAutomata;\n  var posTransformers;\n  var posSetter;\n  var posUpdater;\n  // Patterns or Custom Entities\n  var cerAutomata;\n  var cerTransformer;\n  var cerLearnings = 0;\n  var cerPreserve;\n  var cerConfig;\n  // Used for compiling examples.\n  var compiler;\n  // Used to innstantiate the compiler.\n  var cerMetaModel;\n\n  // Contains a list of valid annotations built from `theModel`.\n  var validAnnotations = Object.create(null);\n\n  // Current pipe.\n  var currPipe = Object.create(null);\n  var onlyTokenization = true;\n\n  // Private methods.\n\n  // ## load\n  /**\n   * Loads the model containing the core model along with other applicable\n   * models.\n   * @private\n   *\n   * @returns {void} nothing!.\n   * @private\n  */\n  var load = function () {\n    // Load language model.\n    model = theModel.core();\n    // With `intrinsicSize` captured, instantiate cache etc.\n    cache = Cache(model, theModel.featureFn); // eslint-disable-line new-cap\n    trex = compileTRex(model.trex);\n\n    // Instantiate tokenizer.\n    tokenize = tokenizer(trex, model.tcat.hash, model.preserve);\n\n    // Load & setup SBD model.\n    var sbdModel = theModel.sbd();\n    sbdAutomata = new Array(sbdModel.machines.length);\n    sbdTransformers = new Array(sbdModel.machines.length);\n    for (let i = 0; i < sbdModel.machines.length; i += 1) {\n      sbdAutomata[i] = fsm(cache);\n      sbdAutomata[i].importJSON(sbdModel.machines[i]);\n      sbdTransformers[i] = sbdModel.transformers[i];\n    }\n    sbdSetter = sbdModel.setter;\n\n    // Load & setup NER model.\n    var nerModel = theModel.ner();\n    nerAutomata = new Array(nerModel.machines.length);\n    nerTransformers = new Array(nerModel.machines.length);\n    for (let i = 0; i < nerModel.machines.length; i += 1) {\n      nerAutomata[i] = fsm(cache);\n      nerAutomata[i].importJSON(nerModel.machines[i]);\n      nerTransformers[i] = nerModel.transformers[i];\n    }\n    var negModel = theModel.negation();\n    negAutomata = fsm(cache);\n    negAutomata.importJSON(negModel.machines[0]);\n    negSetter = negModel.setter;\n    var saModel = theModel.sa();\n    saAutomata = fsm(cache);\n    saAutomata.importJSON(saModel.machines[0]);\n    saSetter = saModel.setter;\n    var posModel = theModel.pos();\n    posAutomata = new Array(posModel.machines.length);\n    posTransformers = new Array(nerModel.machines.length);\n    for (let i = 0; i < posModel.machines.length; i += 1) {\n      // Ignore only OOV literal and not new line character in the case of POS Tagging.\n      posAutomata[i] = fsm(cache, cache.value(0));\n      posAutomata[i].importJSON(posModel.machines[i]);\n      posTransformers[i] = posModel.transformers[i];\n    }\n    posSetter = posModel.setter;\n    posUpdater = posModel.updater;\n    var cmModel = theModel.metaCER();\n    cerMetaModel = cmModel.machines;\n    cerTransformer = cmModel.transformers[0];\n    // posAutomata = fsm( cache, cache.value( 0 ) );\n    // posAutomata.importJSON( posModel.machines[ 0 ] );\n    // posTransformer = posModel.transformers[ 0 ];\n  }; // load()\n\n  // Public Methods.\n  // ## readDoc\n  /**\n   * Loads a single document to be processed.\n   * @private\n   *\n   * @param {string} text of the document that you want to process.\n   * @returns {object} the document in terms of an object that exposes the API.\n   * @example\n   * const DOC = \"The quick brown fox jumps over the lazy dog\";\n   * myNLP.readDoc(DOC);\n  */\n  var readDoc = function (text) {\n    if (typeof text !== 'string') {\n      throw Error(`wink-nlp: expecting a valid Javascript string, instead found \"${typeof text}\".`);\n    }\n    // Raw Document Data-structure gets populated here as NLP pipe taks execute!\n    var rdd = Object.create(null);\n    // The `cache` is also part of document data structure.\n    rdd.cache = cache;\n    // Each document gets a pointer to the word vectors.\n    rdd.wordVectors = wordEmbeddings;\n    // Document's tokens; each token is represented as an array of numbers:\n    // ```\n    // [\n    //   hash, // of tokenized lexeme\n    //   (nox) + preceding spaces, // expansion's normal\n    //   pos + lemma, // pos & lemma are contextual\n    //   negation flag // 1 bit at msb\n    // ]\n    // ```\n    rdd.tokens = [];\n    // Sentences — stored as array of pairs of `[ start, end ]` pointing to the `tokens`.\n    rdd.sentences = [];\n    // Markings are 4-tuples of `start`, `end` **token indexes**,  and `begin & end markers`.\n    // The begin & end markers are used to markup the tokens specified.\n    rdd.markings = [];\n    // Publish the current annotation pipeline so that code can inquire about\n    // active annotations!\n    rdd.currPipe = currPipe;\n    // Set storage for non braking spaces\n    rdd.nonBreakingSpaces = Object.create(null);\n    var wrappedDocData = DocDataWrapper(rdd); // eslint-disable-line new-cap\n\n    // Start of NLP Pipe\n    tokenize(wrappedDocData, text); // eslint-disable-line new-cap\n    // Compute number of tokens.\n    rdd.numOfTokens = rdd.tokens.length / tkSize;\n    // This structure is identical to sentences ( or entities ), for the sake of uniformity.\n    // The structure is `[ start, end, negationFlag, sentimentScore ]`.\n    rdd.document = [0, rdd.numOfTokens - 1, 0, 0];\n\n    // Map tokens for automata if there are other annotations to be performed.\n    var tokens4Automata = onlyTokenization ? null : mapRawTokens2UIdOfNormal(rdd);\n    var px;\n    if (currPipe.sbd) {\n      // Sentence Boundary Detection.\n      // Set first `Pattern Swap (x)` as `null`.\n      px = null;\n      for (let i = 0; i < sbdAutomata.length; i += 1) {\n        sbdAutomata[i].setPatternSwap(px);\n        // For SBD, all tokens are required to extract preceeding spaces.\n        px = sbdAutomata[i].recognize(tokens4Automata, sbdTransformers[i], rdd.tokens);\n      }\n      // The structure of sentence is:<br/>\n      // `[ start, end, negationFlag, sentimentScore ]`\n      sbdSetter(px, rdd);\n      // Compute number of sentences!\n      rdd.numOfSentences = rdd.sentences.length;\n    } else {\n      // Setup default sentence as entire document!\n      rdd.numOfSentences = 1;\n      rdd.sentences = [[0, rdd.numOfTokens - 1, 0, 0]];\n    }\n    if (currPipe.ner) {\n      // Named entity detection.\n      px = null;\n      for (let i = 0; i < nerAutomata.length; i += 1) {\n        nerAutomata[i].setPatternSwap(px);\n        px = nerAutomata[i].recognize(tokens4Automata, nerTransformers[i]);\n      }\n      // Entities — storted as array of `[ start, end, entity type ].`\n      // There are no setter for entities as no transformation is needed.\n      rdd.entities = px;\n    } else {\n      rdd.entities = [];\n    }\n    if (currPipe.negation) {\n      // Negation\n      px = null;\n      px = negAutomata.recognize(tokens4Automata);\n      negSetter(px, rdd, constants, search);\n    }\n    if (currPipe.sentiment) {\n      // Sentiment Analysis\n      px = null;\n      px = saAutomata.recognize(tokens4Automata);\n      saSetter(px, rdd, constants, locate);\n    }\n    if (currPipe.pos) {\n      // PoS Tagging\n      const posTags = mapRawTokens2UIdOfDefaultPOS(rdd);\n      px = null;\n      for (let i = 0; i < posAutomata.length; i += 1) {\n        px = posAutomata[i].recognize(posTags, posTransformers[0], rdd.tokens);\n        posUpdater(px, cache, posTags, tokens4Automata);\n      }\n      posSetter(rdd, posTags, tkSize, constants.bits4lemma);\n    }\n    if (currPipe.cer) {\n      // Patterns\n      px = null;\n      if (cerAutomata !== undefined && cerLearnings > 0) {\n        cerConfig.rdd = rdd;\n        cerConfig.preserve = cerPreserve;\n        cerConfig.constants = constants;\n        if (cerConfig.useEntity) cerAutomata.setPatternSwap(rdd.entities);\n        px = cerAutomata.recognize(tokens4Automata, cerTransformer, cerConfig);\n      }\n      // If there are no custom entities, then `px` will be `null`; in such a case\n      // set `customEntities` to an empty array.\n      rdd.customEntities = px || [];\n    } else rdd.customEntities = [];\n\n    // Word Vector\n    // if ( theModel.wordVectors !== undefined ) {\n    //\n    // }\n\n    // Now create the document!\n    var doc = Doc(rdd, theModel.addons); // eslint-disable-line new-cap\n\n    // All done — cleanup document's data.\n    wrappedDocData.clean();\n    return doc;\n  }; // readDoc()\n\n  var learnCustomEntities = function (examples, config) {\n    // Ensure (a) `examples` is an array and (b) and its each element is an object.\n    if (helper.isArray(examples)) {\n      examples.forEach(ex => {\n        if (helper.isObject(ex)) {\n          // The object must contain name  & patterns property of string and array type respectively.\n          if (typeof ex.name !== 'string' || ex.name === '') {\n            throw Error(`wink-nlp: name should be a string, instead found \"${ex.name}\":\\n\\n${JSON.stringify(ex, null, 2)}`);\n          } else if (helper.isArray(ex.patterns)) {\n            for (let k = 0; k < ex.patterns.length; k += 1) {\n              const p = ex.patterns[k];\n              // Each pattern should be a string.\n              if (typeof p !== 'string' || p === '') {\n                throw Error(`wink-nlp: each pattern should be a string, instead found \"${p}\":\\n\\n${JSON.stringify(ex, null, 2)}`);\n              }\n            } // for ( let k = 0;... )\n          } else {\n            // Pattern is not an array.\n            throw Error(`wink-nlp: patterns should be an array, instead found \"${typeof ex.patterns}\":\\n\\n${JSON.stringify(ex, null, 2)}`);\n          }\n          // If mark is present then it should be an array of integers **and** its length must\n          // be equal to 2 **and** start index <= end index.\n          if (ex.mark !== undefined && (!helper.isIntegerArray(ex.mark) || ex.mark.length !== 2 || ex.mark.length === 2 && ex.mark[0] > ex.mark[1])) {\n            throw Error(`wink-nlp: mark should be an array containing start & end indexes, instead found:\\n\\n${JSON.stringify(ex.mark, null, 2)}`);\n          }\n        } else {\n          // Example is not an object.\n          throw Error(`wink-nlp: each example should be an object, instead found a \"${typeof ex}\":\\n\\n${JSON.stringify(ex, null, 2)}`);\n        }\n      });\n    } else {\n      // Examples is not an array.\n      throw Error(`wink-nlp: examples should be an array, instead found \"${typeof examples}\".`);\n    }\n\n    // Validate config\n    cerConfig = config === undefined || config === null ? Object.create(null) : JSON.parse(JSON.stringify(config));\n    if (!helper.isObject(cerConfig)) {\n      throw Error(`wink-nlp: config should be an object, instead found \"${typeof cerConfig}\".`);\n    }\n    cerConfig.matchValue = !!cerConfig.matchValue;\n    cerConfig.usePOS = cerConfig.usePOS === undefined ? true : !!cerConfig.usePOS;\n    cerConfig.useEntity = cerConfig.useEntity === undefined ? true : !!cerConfig.useEntity;\n\n    // Instantiate compiler.\n    compiler = Compiler(cerMetaModel, cache, tokenize, cerConfig.matchValue); // eslint-disable-line new-cap\n\n    cerAutomata = null;\n    cerLearnings = 0;\n    cerAutomata = fsm();\n    const compiled = compiler.run(examples);\n    cerPreserve = compiled.preserve;\n    cerLearnings = cerAutomata.learn(compiled.examples);\n    // cerAutomata.printModel();\n    return cerLearnings;\n  }; // learnCustomEntities()\n\n  if (helper.isObject(theModel)) {\n    if (typeof theModel.core !== 'function') {\n      throw Error('wink-nlp: invalid model used.');\n    }\n  } else {\n    throw Error('wink-nlp: invalid model used.');\n  }\n\n  // Build a list of valid annotations from `theModel`. This will ensure that\n  // only **available** annotations from the model can be used in the pipe.\n  validAnnotations.sbd = typeof theModel.sbd === 'function';\n  validAnnotations.negation = typeof theModel.negation === 'function';\n  validAnnotations.sentiment = typeof theModel.sa === 'function';\n  validAnnotations.pos = typeof theModel.pos === 'function';\n  validAnnotations.ner = typeof theModel.ner === 'function';\n  validAnnotations.cer = typeof theModel.metaCER === 'function';\n  if (wordEmbeddings !== null) {\n    if (!helper.isObject(wordEmbeddings)) throw Error(`wink-nlp: invalid word vectors, it must be an object instead found a \"${typeof wordEmbeddings}\".`);\n    let numOfKeys = 0;\n    const wordVectorKeys = Object.create(null);\n    wordVectorKeys.precision = true;\n    wordVectorKeys.l2NormIndex = true;\n    wordVectorKeys.wordIndex = true;\n    wordVectorKeys.dimensions = true;\n    wordVectorKeys.unkVector = true;\n    wordVectorKeys.size = true;\n    wordVectorKeys.words = true;\n    wordVectorKeys.vectors = true;\n    for (const key in wordEmbeddings) {\n      // eslint-disable-line guard-for-in\n      numOfKeys += 1;\n      if (!wordVectorKeys[key]) throw Error('wink-nlp: invalid word vectors format.');\n    }\n    if (numOfKeys === 0) throw Error('wink-nlp: empty word vectors found.');\n  }\n  const tempPipe = pipe === null || pipe === undefined ? Object.keys(validAnnotations) : pipe;\n  if (helper.isArray(tempPipe)) {\n    tempPipe.forEach(at => {\n      if (!validAnnotations[at]) throw Error(`wink-nlp: invalid pipe annotation \"${at}\" found.`);\n      currPipe[at] = true;\n      onlyTokenization = false;\n    });\n  } else throw Error(`wink-nlp: invalid pipe, it must be an array instead found a \"${typeof pipe}\".`);\n\n  // Load the model.\n  load();\n  // Setup default configuration.\n  // definePipeConfig();\n  // Methods.\n  methods.readDoc = readDoc;\n  methods.learnCustomEntities = learnCustomEntities;\n  // Expose `its` and `as` helpers.\n  methods.its = itsHelpers;\n  methods.as = asHelpers;\n  // Vector of a token method.\n  methods.vectorOf = function (word, safe = true) {\n    if (!wordEmbeddings) throw Error('wink-nlp: word vectors are not loaded, use const nlp = winkNLP( model, pipe, wordVectors ) to load.');\n    const vectors = wordEmbeddings.vectors;\n    const unkVector = wordEmbeddings.unkVector;\n    const sliceUpTo = wordEmbeddings.l2NormIndex + 1;\n    if (typeof word !== 'string') {\n      throw Error('winkNLP: input word must be of type string.');\n    }\n    const tv = vectors[word.toLowerCase()];\n    if (tv === undefined) {\n      // If unsafe, return the entire array.\n      return safe ? unkVector.slice(0, sliceUpTo) : unkVector.slice();\n    }\n    return safe ? tv.slice(0, sliceUpTo) : tv.slice();\n  }; // vectorOf()\n\n  return methods;\n}; // wink\n\nmodule.exports = nlp;","map":{"version":3,"names":["DocDataWrapper","require","Doc","Cache","tokenizer","compileTRex","mappers","itsHelpers","asHelpers","mapRawTokens2UIdOfNormal","mapRawTokens2UIdOfDefaultPOS","Compiler","constants","fsm","search","locate","helper","tkSize","nlp","theModel","pipe","wordEmbeddings","methods","Object","create","trex","model","cache","tokenize","nerAutomata","nerTransformers","sbdAutomata","sbdTransformers","sbdSetter","negAutomata","negSetter","saAutomata","saSetter","posAutomata","posTransformers","posSetter","posUpdater","cerAutomata","cerTransformer","cerLearnings","cerPreserve","cerConfig","compiler","cerMetaModel","validAnnotations","currPipe","onlyTokenization","load","core","featureFn","tcat","hash","preserve","sbdModel","sbd","Array","machines","length","i","importJSON","transformers","setter","nerModel","ner","negModel","negation","saModel","sa","posModel","pos","value","updater","cmModel","metaCER","readDoc","text","Error","rdd","wordVectors","tokens","sentences","markings","nonBreakingSpaces","wrappedDocData","numOfTokens","document","tokens4Automata","px","setPatternSwap","recognize","numOfSentences","entities","sentiment","posTags","bits4lemma","cer","undefined","useEntity","customEntities","doc","addons","clean","learnCustomEntities","examples","config","isArray","forEach","ex","isObject","name","JSON","stringify","patterns","k","p","mark","isIntegerArray","parse","matchValue","usePOS","compiled","run","learn","numOfKeys","wordVectorKeys","precision","l2NormIndex","wordIndex","dimensions","unkVector","size","words","vectors","key","tempPipe","keys","at","its","as","vectorOf","word","safe","sliceUpTo","tv","toLowerCase","slice","module","exports"],"sources":["C:/Users/cheko/Desktop/Education/Freelance/criticowl-main/criticowl_frontend/node_modules/wink-nlp/src/wink-nlp.js"],"sourcesContent":["//     wink-nlp\n//\n//     Copyright (C) GRAYPE Systems Private Limited\n//\n//     This file is part of “wink-nlp”.\n//\n//     Permission is hereby granted, free of charge, to any\n//     person obtaining a copy of this software and\n//     associated documentation files (the \"Software\"), to\n//     deal in the Software without restriction, including\n//     without limitation the rights to use, copy, modify,\n//     merge, publish, distribute, sublicense, and/or sell\n//     copies of the Software, and to permit persons to\n//     whom the Software is furnished to do so, subject to\n//     the following conditions:\n//\n//     The above copyright notice and this permission notice\n//     shall be included in all copies or substantial\n//     portions of the Software.\n//\n//     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF\n//     ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n//     TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n//     PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n//     THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n//     DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n//     CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n//     CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n//     DEALINGS IN THE SOFTWARE.\n\n//\n\nvar DocDataWrapper = require( './dd-wrapper.js' );\nvar Doc = require( './doc-v2.js' );\nvar Cache = require( './cache.js' );\nvar tokenizer = require( './tokenizer.js' );\nvar compileTRex = require( './compile-trex.js' );\nvar mappers = require( './tokens-mappers.js' );\nvar itsHelpers = require( './its.js' );\nvar asHelpers = require( './as.js' );\nvar mapRawTokens2UIdOfNormal = mappers.mapRawTokens2UIdOfNormal;\nvar mapRawTokens2UIdOfDefaultPOS = mappers.mapRawTokens2UIdOfDefaultPOS;\n\nvar Compiler = require( './examples-compiler.js' );\n\nvar constants = require( './constants.js' );\n\nvar fsm = require( './automaton.js' );\n\nvar search = require( './search.js' );\nvar locate = require( './locate.js' );\n\nvar helper = require( './helper.js' );\n\n// Size of a single token.\nvar tkSize = constants.tkSize;\n\n/**\n * Creates an instance of nlp.\n * @private\n *\n * @param {object} theModel language model.\n * @param {string[]} pipe of nlp annotations.\n * @param {object} wordEmbeddings object read using node require.\n * @returns {object} conatining set of API methods for natural language processing.\n * @example\n * const nlp = require( 'wink-nlp' );\n * var myNLP = nlp();\n*/\nvar nlp = function ( theModel, pipe = null, wordEmbeddings = null ) {\n\n  var methods = Object.create( null );\n  // Token Regex; compiled from `model`\n  var trex;\n  // wink-nlp language `model`.\n  var model;\n  // Holds instance of `cache` created using the `model`.\n  var cache;\n  // NLP Pipe Config.\n  // var nlpPipe = Object.create( null );\n  // Configured tokenize.\n  var tokenize;\n  // Automata\n  // 1. NER\n  var nerAutomata;\n  var nerTransformers;\n  // 2. SBD\n  var sbdAutomata;\n  var sbdTransformers;\n  var sbdSetter;\n  // 3. NEG\n  var negAutomata;\n  var negSetter;\n  // SA\n  var saAutomata;\n  var saSetter;\n  // POS\n  var posAutomata;\n  var posTransformers;\n  var posSetter;\n  var posUpdater;\n  // Patterns or Custom Entities\n  var cerAutomata;\n  var cerTransformer;\n  var cerLearnings = 0;\n  var cerPreserve;\n  var cerConfig;\n  // Used for compiling examples.\n  var compiler;\n  // Used to innstantiate the compiler.\n  var cerMetaModel;\n\n  // Contains a list of valid annotations built from `theModel`.\n  var validAnnotations = Object.create( null );\n\n  // Current pipe.\n  var currPipe = Object.create( null );\n  var onlyTokenization = true;\n\n  // Private methods.\n\n  // ## load\n  /**\n   * Loads the model containing the core model along with other applicable\n   * models.\n   * @private\n   *\n   * @returns {void} nothing!.\n   * @private\n  */\n  var load = function () {\n    // Load language model.\n    model = theModel.core();\n    // With `intrinsicSize` captured, instantiate cache etc.\n    cache = Cache( model, theModel.featureFn ); // eslint-disable-line new-cap\n    trex = compileTRex( model.trex );\n\n    // Instantiate tokenizer.\n    tokenize = tokenizer( trex, model.tcat.hash, model.preserve );\n\n    // Load & setup SBD model.\n    var sbdModel = theModel.sbd();\n\n    sbdAutomata = new Array( sbdModel.machines.length );\n    sbdTransformers = new Array( sbdModel.machines.length );\n    for ( let i = 0; i < sbdModel.machines.length; i += 1 ) {\n      sbdAutomata[ i ] = fsm( cache );\n      sbdAutomata[ i ].importJSON( sbdModel.machines[ i ] );\n      sbdTransformers[ i ] = sbdModel.transformers[ i ];\n    }\n    sbdSetter = sbdModel.setter;\n\n    // Load & setup NER model.\n    var nerModel = theModel.ner();\n\n    nerAutomata = new Array( nerModel.machines.length );\n    nerTransformers = new Array( nerModel.machines.length );\n    for ( let i = 0; i < nerModel.machines.length; i += 1 ) {\n      nerAutomata[ i ] = fsm( cache );\n      nerAutomata[ i ].importJSON( nerModel.machines[ i ] );\n      nerTransformers[ i ] = nerModel.transformers[ i ];\n    }\n\n    var negModel = theModel.negation();\n    negAutomata = fsm( cache );\n    negAutomata.importJSON( negModel.machines[ 0 ] );\n    negSetter = negModel.setter;\n\n    var saModel = theModel.sa();\n    saAutomata = fsm( cache );\n    saAutomata.importJSON( saModel.machines[ 0 ] );\n    saSetter = saModel.setter;\n\n    var posModel = theModel.pos();\n    posAutomata = new Array( posModel.machines.length );\n    posTransformers = new Array( nerModel.machines.length );\n    for ( let i = 0; i < posModel.machines.length; i += 1 ) {\n      // Ignore only OOV literal and not new line character in the case of POS Tagging.\n      posAutomata[ i ] = fsm( cache, cache.value( 0 ) );\n      posAutomata[ i ].importJSON( posModel.machines[ i ] );\n      posTransformers[ i ] = posModel.transformers[ i ];\n    }\n    posSetter = posModel.setter;\n    posUpdater = posModel.updater;\n\n\n    var cmModel = theModel.metaCER();\n    cerMetaModel = cmModel.machines;\n    cerTransformer = cmModel.transformers[ 0 ];\n    // posAutomata = fsm( cache, cache.value( 0 ) );\n    // posAutomata.importJSON( posModel.machines[ 0 ] );\n    // posTransformer = posModel.transformers[ 0 ];\n  }; // load()\n\n  // Public Methods.\n  // ## readDoc\n  /**\n   * Loads a single document to be processed.\n   * @private\n   *\n   * @param {string} text of the document that you want to process.\n   * @returns {object} the document in terms of an object that exposes the API.\n   * @example\n   * const DOC = \"The quick brown fox jumps over the lazy dog\";\n   * myNLP.readDoc(DOC);\n  */\n  var readDoc = function ( text ) {\n    if ( typeof text !== 'string' ) {\n      throw Error( `wink-nlp: expecting a valid Javascript string, instead found \"${typeof text}\".`);\n    }\n    // Raw Document Data-structure gets populated here as NLP pipe taks execute!\n    var rdd = Object.create( null );\n    // The `cache` is also part of document data structure.\n    rdd.cache = cache;\n    // Each document gets a pointer to the word vectors.\n    rdd.wordVectors = wordEmbeddings;\n    // Document's tokens; each token is represented as an array of numbers:\n    // ```\n    // [\n    //   hash, // of tokenized lexeme\n    //   (nox) + preceding spaces, // expansion's normal\n    //   pos + lemma, // pos & lemma are contextual\n    //   negation flag // 1 bit at msb\n    // ]\n    // ```\n    rdd.tokens = [];\n    // Sentences — stored as array of pairs of `[ start, end ]` pointing to the `tokens`.\n    rdd.sentences = [];\n    // Markings are 4-tuples of `start`, `end` **token indexes**,  and `begin & end markers`.\n    // The begin & end markers are used to markup the tokens specified.\n    rdd.markings = [];\n    // Publish the current annotation pipeline so that code can inquire about\n    // active annotations!\n    rdd.currPipe = currPipe;\n    // Set storage for non braking spaces\n    rdd.nonBreakingSpaces = Object.create( null );\n\n    var wrappedDocData = DocDataWrapper( rdd );  // eslint-disable-line new-cap\n\n    // Start of NLP Pipe\n    tokenize( wrappedDocData, text ); // eslint-disable-line new-cap\n    // Compute number of tokens.\n    rdd.numOfTokens = rdd.tokens.length / tkSize;\n    // This structure is identical to sentences ( or entities ), for the sake of uniformity.\n    // The structure is `[ start, end, negationFlag, sentimentScore ]`.\n    rdd.document = [ 0, ( rdd.numOfTokens - 1 ), 0, 0 ];\n\n    // Map tokens for automata if there are other annotations to be performed.\n    var tokens4Automata = ( onlyTokenization ) ? null : mapRawTokens2UIdOfNormal( rdd );\n\n    var px;\n    if ( currPipe.sbd ) {\n      // Sentence Boundary Detection.\n      // Set first `Pattern Swap (x)` as `null`.\n      px = null;\n      for ( let i = 0; i < sbdAutomata.length; i += 1 ) {\n        sbdAutomata[ i ].setPatternSwap( px );\n        // For SBD, all tokens are required to extract preceeding spaces.\n        px = sbdAutomata[ i ].recognize( tokens4Automata, sbdTransformers[ i ], rdd.tokens );\n      }\n      // The structure of sentence is:<br/>\n      // `[ start, end, negationFlag, sentimentScore ]`\n      sbdSetter( px, rdd );\n      // Compute number of sentences!\n      rdd.numOfSentences = rdd.sentences.length;\n    } else {\n      // Setup default sentence as entire document!\n      rdd.numOfSentences = 1;\n      rdd.sentences = [ [ 0, ( rdd.numOfTokens - 1 ), 0, 0 ] ];\n    }\n\n    if ( currPipe.ner ) {\n      // Named entity detection.\n      px = null;\n      for ( let i = 0; i < nerAutomata.length; i += 1 ) {\n        nerAutomata[ i ].setPatternSwap( px );\n        px = nerAutomata[ i ].recognize( tokens4Automata, nerTransformers[ i ] );\n      }\n      // Entities — storted as array of `[ start, end, entity type ].`\n      // There are no setter for entities as no transformation is needed.\n      rdd.entities = px;\n    } else {\n      rdd.entities = [];\n    }\n\n    if ( currPipe.negation ) {\n      // Negation\n      px = null;\n      px = negAutomata.recognize( tokens4Automata );\n      negSetter( px, rdd, constants, search );\n    }\n\n    if ( currPipe.sentiment ) {\n      // Sentiment Analysis\n      px = null;\n      px = saAutomata.recognize( tokens4Automata );\n      saSetter( px, rdd, constants, locate );\n    }\n\n    if ( currPipe.pos ) {\n      // PoS Tagging\n      const posTags = mapRawTokens2UIdOfDefaultPOS( rdd );\n      px = null;\n      for ( let i = 0; i < posAutomata.length; i += 1 ) {\n        px = posAutomata[ i ].recognize( posTags, posTransformers[ 0 ], rdd.tokens );\n        posUpdater( px, cache, posTags, tokens4Automata );\n      }\n      posSetter( rdd, posTags, tkSize, constants.bits4lemma );\n    }\n\n    if ( currPipe.cer ) {\n      // Patterns\n      px = null;\n      if ( cerAutomata !== undefined && cerLearnings > 0 ) {\n        cerConfig.rdd = rdd;\n        cerConfig.preserve = cerPreserve;\n        cerConfig.constants = constants;\n        if ( cerConfig.useEntity ) cerAutomata.setPatternSwap( rdd.entities );\n        px = cerAutomata.recognize( tokens4Automata, cerTransformer, cerConfig );\n      }\n      // If there are no custom entities, then `px` will be `null`; in such a case\n      // set `customEntities` to an empty array.\n      rdd.customEntities = px || [];\n    } else rdd.customEntities = [];\n\n\n    // Word Vector\n    // if ( theModel.wordVectors !== undefined ) {\n    //\n    // }\n\n    // Now create the document!\n    var doc = Doc( rdd, theModel.addons ); // eslint-disable-line new-cap\n\n    // All done — cleanup document's data.\n    wrappedDocData.clean();\n    return doc;\n  }; // readDoc()\n\n  var learnCustomEntities = function ( examples, config ) {\n    // Ensure (a) `examples` is an array and (b) and its each element is an object.\n    if ( helper.isArray( examples ) ) {\n      examples.forEach( ( ex ) => {\n        if ( helper.isObject( ex ) ) {\n          // The object must contain name  & patterns property of string and array type respectively.\n          if ( ( typeof ex.name !== 'string' ) || ( ex.name === '' ) ) {\n            throw Error( `wink-nlp: name should be a string, instead found \"${ex.name}\":\\n\\n${JSON.stringify( ex, null, 2 )}` );\n          } else if ( helper.isArray( ex.patterns ) ) {\n            for ( let k = 0; k < ex.patterns.length; k += 1 ) {\n              const p = ex.patterns[ k ];\n              // Each pattern should be a string.\n              if ( ( typeof p !== 'string' ) || ( p === '' ) ) {\n                throw Error( `wink-nlp: each pattern should be a string, instead found \"${p}\":\\n\\n${JSON.stringify( ex, null, 2 )}` );\n              }\n            } // for ( let k = 0;... )\n          } else {\n            // Pattern is not an array.\n            throw Error( `wink-nlp: patterns should be an array, instead found \"${typeof ex.patterns}\":\\n\\n${JSON.stringify( ex, null, 2 )}` );\n          }\n          // If mark is present then it should be an array of integers **and** its length must\n          // be equal to 2 **and** start index <= end index.\n          if ( ( ex.mark !== undefined ) &&\n                ( !helper.isIntegerArray( ex.mark ) ||\n                ( ex.mark.length !== 2 ) ||\n                ( ex.mark.length === 2 && ex.mark[ 0 ] > ex.mark[ 1 ] ) ) ) {\n            throw Error( `wink-nlp: mark should be an array containing start & end indexes, instead found:\\n\\n${JSON.stringify( ex.mark, null, 2 )}` );\n          }\n        } else {\n          // Example is not an object.\n          throw Error( `wink-nlp: each example should be an object, instead found a \"${typeof ex}\":\\n\\n${JSON.stringify( ex, null, 2 )}` );\n        }\n      } );\n    } else {\n      // Examples is not an array.\n      throw Error( `wink-nlp: examples should be an array, instead found \"${typeof examples}\".` );\n    }\n\n    // Validate config\n    cerConfig = ( config === undefined || config === null ) ? Object.create( null ) : JSON.parse( JSON.stringify( config ) );\n    if ( !helper.isObject( cerConfig ) ) {\n      throw Error( `wink-nlp: config should be an object, instead found \"${typeof cerConfig}\".` );\n    }\n    cerConfig.matchValue = !!cerConfig.matchValue;\n    cerConfig.usePOS = ( cerConfig.usePOS === undefined ) ? true : !!cerConfig.usePOS;\n    cerConfig.useEntity = ( cerConfig.useEntity === undefined ) ? true : !!cerConfig.useEntity;\n\n\n    // Instantiate compiler.\n    compiler = Compiler( cerMetaModel, cache, tokenize, cerConfig.matchValue ); // eslint-disable-line new-cap\n\n    cerAutomata = null;\n    cerLearnings = 0;\n    cerAutomata = fsm();\n    const compiled = compiler.run( examples );\n    cerPreserve = compiled.preserve;\n    cerLearnings = cerAutomata.learn( compiled.examples );\n    // cerAutomata.printModel();\n    return cerLearnings;\n  }; // learnCustomEntities()\n\n  if ( helper.isObject( theModel ) ) {\n    if ( typeof theModel.core !== 'function' ) {\n      throw Error( 'wink-nlp: invalid model used.' );\n    }\n  } else {\n    throw Error( 'wink-nlp: invalid model used.' );\n  }\n\n  // Build a list of valid annotations from `theModel`. This will ensure that\n  // only **available** annotations from the model can be used in the pipe.\n  validAnnotations.sbd = typeof theModel.sbd === 'function';\n  validAnnotations.negation = typeof theModel.negation === 'function';\n  validAnnotations.sentiment = typeof theModel.sa === 'function';\n  validAnnotations.pos = typeof theModel.pos === 'function';\n  validAnnotations.ner = typeof theModel.ner === 'function';\n  validAnnotations.cer = typeof theModel.metaCER === 'function';\n\n  if ( wordEmbeddings !== null ) {\n    if ( !helper.isObject( wordEmbeddings ) )\n      throw Error( `wink-nlp: invalid word vectors, it must be an object instead found a \"${typeof wordEmbeddings}\".` );\n\n    let numOfKeys = 0;\n    const wordVectorKeys = Object.create( null );\n    wordVectorKeys.precision = true;\n    wordVectorKeys.l2NormIndex = true;\n    wordVectorKeys.wordIndex = true;\n    wordVectorKeys.dimensions = true;\n    wordVectorKeys.unkVector = true;\n    wordVectorKeys.size = true;\n    wordVectorKeys.words = true;\n    wordVectorKeys.vectors = true;\n    for ( const key in wordEmbeddings ) { // eslint-disable-line guard-for-in\n      numOfKeys += 1;\n      if ( !wordVectorKeys[ key ] )\n        throw Error( 'wink-nlp: invalid word vectors format.' );\n    }\n\n    if ( numOfKeys === 0 ) throw Error( 'wink-nlp: empty word vectors found.' );\n  }\n\n  const tempPipe = ( pipe === null || pipe === undefined ) ? Object.keys( validAnnotations ) : pipe;\n  if ( helper.isArray( tempPipe ) ) {\n    tempPipe.forEach( ( at ) => {\n      if ( !validAnnotations[ at ] ) throw Error( `wink-nlp: invalid pipe annotation \"${at}\" found.` );\n      currPipe[ at ] = true;\n      onlyTokenization = false;\n    } );\n  } else throw Error( `wink-nlp: invalid pipe, it must be an array instead found a \"${typeof pipe}\".` );\n\n  // Load the model.\n  load();\n  // Setup default configuration.\n  // definePipeConfig();\n  // Methods.\n  methods.readDoc = readDoc;\n  methods.learnCustomEntities = learnCustomEntities;\n  // Expose `its` and `as` helpers.\n  methods.its = itsHelpers;\n  methods.as = asHelpers;\n  // Vector of a token method.\n  methods.vectorOf = function ( word, safe = true )  {\n    if ( !wordEmbeddings )\n    throw Error( 'wink-nlp: word vectors are not loaded, use const nlp = winkNLP( model, pipe, wordVectors ) to load.' );\n\n    const vectors = wordEmbeddings.vectors;\n    const unkVector = wordEmbeddings.unkVector;\n    const sliceUpTo = wordEmbeddings.l2NormIndex + 1;\n\n    if ( typeof word !== 'string' ) {\n      throw Error( 'winkNLP: input word must be of type string.' );\n    }\n\n    const tv = vectors[ word.toLowerCase() ];\n    if ( tv === undefined ) {\n      // If unsafe, return the entire array.\n      return ( safe ) ? unkVector.slice( 0, sliceUpTo ) : unkVector.slice();\n    }\n    return ( safe ) ? tv.slice( 0, sliceUpTo ) : tv.slice();\n  }; // vectorOf()\n\n  return methods;\n}; // wink\n\nmodule.exports = nlp;\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA,IAAIA,cAAc,GAAGC,OAAO,CAAE,iBAAkB,CAAC;AACjD,IAAIC,GAAG,GAAGD,OAAO,CAAE,aAAc,CAAC;AAClC,IAAIE,KAAK,GAAGF,OAAO,CAAE,YAAa,CAAC;AACnC,IAAIG,SAAS,GAAGH,OAAO,CAAE,gBAAiB,CAAC;AAC3C,IAAII,WAAW,GAAGJ,OAAO,CAAE,mBAAoB,CAAC;AAChD,IAAIK,OAAO,GAAGL,OAAO,CAAE,qBAAsB,CAAC;AAC9C,IAAIM,UAAU,GAAGN,OAAO,CAAE,UAAW,CAAC;AACtC,IAAIO,SAAS,GAAGP,OAAO,CAAE,SAAU,CAAC;AACpC,IAAIQ,wBAAwB,GAAGH,OAAO,CAACG,wBAAwB;AAC/D,IAAIC,4BAA4B,GAAGJ,OAAO,CAACI,4BAA4B;AAEvE,IAAIC,QAAQ,GAAGV,OAAO,CAAE,wBAAyB,CAAC;AAElD,IAAIW,SAAS,GAAGX,OAAO,CAAE,gBAAiB,CAAC;AAE3C,IAAIY,GAAG,GAAGZ,OAAO,CAAE,gBAAiB,CAAC;AAErC,IAAIa,MAAM,GAAGb,OAAO,CAAE,aAAc,CAAC;AACrC,IAAIc,MAAM,GAAGd,OAAO,CAAE,aAAc,CAAC;AAErC,IAAIe,MAAM,GAAGf,OAAO,CAAE,aAAc,CAAC;;AAErC;AACA,IAAIgB,MAAM,GAAGL,SAAS,CAACK,MAAM;;AAE7B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAIC,GAAG,GAAG,SAAAA,CAAWC,QAAQ,EAAEC,IAAI,GAAG,IAAI,EAAEC,cAAc,GAAG,IAAI,EAAG;EAElE,IAAIC,OAAO,GAAGC,MAAM,CAACC,MAAM,CAAE,IAAK,CAAC;EACnC;EACA,IAAIC,IAAI;EACR;EACA,IAAIC,KAAK;EACT;EACA,IAAIC,KAAK;EACT;EACA;EACA;EACA,IAAIC,QAAQ;EACZ;EACA;EACA,IAAIC,WAAW;EACf,IAAIC,eAAe;EACnB;EACA,IAAIC,WAAW;EACf,IAAIC,eAAe;EACnB,IAAIC,SAAS;EACb;EACA,IAAIC,WAAW;EACf,IAAIC,SAAS;EACb;EACA,IAAIC,UAAU;EACd,IAAIC,QAAQ;EACZ;EACA,IAAIC,WAAW;EACf,IAAIC,eAAe;EACnB,IAAIC,SAAS;EACb,IAAIC,UAAU;EACd;EACA,IAAIC,WAAW;EACf,IAAIC,cAAc;EAClB,IAAIC,YAAY,GAAG,CAAC;EACpB,IAAIC,WAAW;EACf,IAAIC,SAAS;EACb;EACA,IAAIC,QAAQ;EACZ;EACA,IAAIC,YAAY;;EAEhB;EACA,IAAIC,gBAAgB,GAAG1B,MAAM,CAACC,MAAM,CAAE,IAAK,CAAC;;EAE5C;EACA,IAAI0B,QAAQ,GAAG3B,MAAM,CAACC,MAAM,CAAE,IAAK,CAAC;EACpC,IAAI2B,gBAAgB,GAAG,IAAI;;EAE3B;;EAEA;EACA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;EACE,IAAIC,IAAI,GAAG,SAAAA,CAAA,EAAY;IACrB;IACA1B,KAAK,GAAGP,QAAQ,CAACkC,IAAI,CAAC,CAAC;IACvB;IACA1B,KAAK,GAAGxB,KAAK,CAAEuB,KAAK,EAAEP,QAAQ,CAACmC,SAAU,CAAC,CAAC,CAAC;IAC5C7B,IAAI,GAAGpB,WAAW,CAAEqB,KAAK,CAACD,IAAK,CAAC;;IAEhC;IACAG,QAAQ,GAAGxB,SAAS,CAAEqB,IAAI,EAAEC,KAAK,CAAC6B,IAAI,CAACC,IAAI,EAAE9B,KAAK,CAAC+B,QAAS,CAAC;;IAE7D;IACA,IAAIC,QAAQ,GAAGvC,QAAQ,CAACwC,GAAG,CAAC,CAAC;IAE7B5B,WAAW,GAAG,IAAI6B,KAAK,CAAEF,QAAQ,CAACG,QAAQ,CAACC,MAAO,CAAC;IACnD9B,eAAe,GAAG,IAAI4B,KAAK,CAAEF,QAAQ,CAACG,QAAQ,CAACC,MAAO,CAAC;IACvD,KAAM,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGL,QAAQ,CAACG,QAAQ,CAACC,MAAM,EAAEC,CAAC,IAAI,CAAC,EAAG;MACtDhC,WAAW,CAAEgC,CAAC,CAAE,GAAGlD,GAAG,CAAEc,KAAM,CAAC;MAC/BI,WAAW,CAAEgC,CAAC,CAAE,CAACC,UAAU,CAAEN,QAAQ,CAACG,QAAQ,CAAEE,CAAC,CAAG,CAAC;MACrD/B,eAAe,CAAE+B,CAAC,CAAE,GAAGL,QAAQ,CAACO,YAAY,CAAEF,CAAC,CAAE;IACnD;IACA9B,SAAS,GAAGyB,QAAQ,CAACQ,MAAM;;IAE3B;IACA,IAAIC,QAAQ,GAAGhD,QAAQ,CAACiD,GAAG,CAAC,CAAC;IAE7BvC,WAAW,GAAG,IAAI+B,KAAK,CAAEO,QAAQ,CAACN,QAAQ,CAACC,MAAO,CAAC;IACnDhC,eAAe,GAAG,IAAI8B,KAAK,CAAEO,QAAQ,CAACN,QAAQ,CAACC,MAAO,CAAC;IACvD,KAAM,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGI,QAAQ,CAACN,QAAQ,CAACC,MAAM,EAAEC,CAAC,IAAI,CAAC,EAAG;MACtDlC,WAAW,CAAEkC,CAAC,CAAE,GAAGlD,GAAG,CAAEc,KAAM,CAAC;MAC/BE,WAAW,CAAEkC,CAAC,CAAE,CAACC,UAAU,CAAEG,QAAQ,CAACN,QAAQ,CAAEE,CAAC,CAAG,CAAC;MACrDjC,eAAe,CAAEiC,CAAC,CAAE,GAAGI,QAAQ,CAACF,YAAY,CAAEF,CAAC,CAAE;IACnD;IAEA,IAAIM,QAAQ,GAAGlD,QAAQ,CAACmD,QAAQ,CAAC,CAAC;IAClCpC,WAAW,GAAGrB,GAAG,CAAEc,KAAM,CAAC;IAC1BO,WAAW,CAAC8B,UAAU,CAAEK,QAAQ,CAACR,QAAQ,CAAE,CAAC,CAAG,CAAC;IAChD1B,SAAS,GAAGkC,QAAQ,CAACH,MAAM;IAE3B,IAAIK,OAAO,GAAGpD,QAAQ,CAACqD,EAAE,CAAC,CAAC;IAC3BpC,UAAU,GAAGvB,GAAG,CAAEc,KAAM,CAAC;IACzBS,UAAU,CAAC4B,UAAU,CAAEO,OAAO,CAACV,QAAQ,CAAE,CAAC,CAAG,CAAC;IAC9CxB,QAAQ,GAAGkC,OAAO,CAACL,MAAM;IAEzB,IAAIO,QAAQ,GAAGtD,QAAQ,CAACuD,GAAG,CAAC,CAAC;IAC7BpC,WAAW,GAAG,IAAIsB,KAAK,CAAEa,QAAQ,CAACZ,QAAQ,CAACC,MAAO,CAAC;IACnDvB,eAAe,GAAG,IAAIqB,KAAK,CAAEO,QAAQ,CAACN,QAAQ,CAACC,MAAO,CAAC;IACvD,KAAM,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGU,QAAQ,CAACZ,QAAQ,CAACC,MAAM,EAAEC,CAAC,IAAI,CAAC,EAAG;MACtD;MACAzB,WAAW,CAAEyB,CAAC,CAAE,GAAGlD,GAAG,CAAEc,KAAK,EAAEA,KAAK,CAACgD,KAAK,CAAE,CAAE,CAAE,CAAC;MACjDrC,WAAW,CAAEyB,CAAC,CAAE,CAACC,UAAU,CAAES,QAAQ,CAACZ,QAAQ,CAAEE,CAAC,CAAG,CAAC;MACrDxB,eAAe,CAAEwB,CAAC,CAAE,GAAGU,QAAQ,CAACR,YAAY,CAAEF,CAAC,CAAE;IACnD;IACAvB,SAAS,GAAGiC,QAAQ,CAACP,MAAM;IAC3BzB,UAAU,GAAGgC,QAAQ,CAACG,OAAO;IAG7B,IAAIC,OAAO,GAAG1D,QAAQ,CAAC2D,OAAO,CAAC,CAAC;IAChC9B,YAAY,GAAG6B,OAAO,CAAChB,QAAQ;IAC/BlB,cAAc,GAAGkC,OAAO,CAACZ,YAAY,CAAE,CAAC,CAAE;IAC1C;IACA;IACA;EACF,CAAC,CAAC,CAAC;;EAEH;EACA;EACA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACE,IAAIc,OAAO,GAAG,SAAAA,CAAWC,IAAI,EAAG;IAC9B,IAAK,OAAOA,IAAI,KAAK,QAAQ,EAAG;MAC9B,MAAMC,KAAK,CAAG,iEAAgE,OAAOD,IAAK,IAAG,CAAC;IAChG;IACA;IACA,IAAIE,GAAG,GAAG3D,MAAM,CAACC,MAAM,CAAE,IAAK,CAAC;IAC/B;IACA0D,GAAG,CAACvD,KAAK,GAAGA,KAAK;IACjB;IACAuD,GAAG,CAACC,WAAW,GAAG9D,cAAc;IAChC;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA6D,GAAG,CAACE,MAAM,GAAG,EAAE;IACf;IACAF,GAAG,CAACG,SAAS,GAAG,EAAE;IAClB;IACA;IACAH,GAAG,CAACI,QAAQ,GAAG,EAAE;IACjB;IACA;IACAJ,GAAG,CAAChC,QAAQ,GAAGA,QAAQ;IACvB;IACAgC,GAAG,CAACK,iBAAiB,GAAGhE,MAAM,CAACC,MAAM,CAAE,IAAK,CAAC;IAE7C,IAAIgE,cAAc,GAAGxF,cAAc,CAAEkF,GAAI,CAAC,CAAC,CAAE;;IAE7C;IACAtD,QAAQ,CAAE4D,cAAc,EAAER,IAAK,CAAC,CAAC,CAAC;IAClC;IACAE,GAAG,CAACO,WAAW,GAAGP,GAAG,CAACE,MAAM,CAACtB,MAAM,GAAG7C,MAAM;IAC5C;IACA;IACAiE,GAAG,CAACQ,QAAQ,GAAG,CAAE,CAAC,EAAIR,GAAG,CAACO,WAAW,GAAG,CAAC,EAAI,CAAC,EAAE,CAAC,CAAE;;IAEnD;IACA,IAAIE,eAAe,GAAKxC,gBAAgB,GAAK,IAAI,GAAG1C,wBAAwB,CAAEyE,GAAI,CAAC;IAEnF,IAAIU,EAAE;IACN,IAAK1C,QAAQ,CAACS,GAAG,EAAG;MAClB;MACA;MACAiC,EAAE,GAAG,IAAI;MACT,KAAM,IAAI7B,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGhC,WAAW,CAAC+B,MAAM,EAAEC,CAAC,IAAI,CAAC,EAAG;QAChDhC,WAAW,CAAEgC,CAAC,CAAE,CAAC8B,cAAc,CAAED,EAAG,CAAC;QACrC;QACAA,EAAE,GAAG7D,WAAW,CAAEgC,CAAC,CAAE,CAAC+B,SAAS,CAAEH,eAAe,EAAE3D,eAAe,CAAE+B,CAAC,CAAE,EAAEmB,GAAG,CAACE,MAAO,CAAC;MACtF;MACA;MACA;MACAnD,SAAS,CAAE2D,EAAE,EAAEV,GAAI,CAAC;MACpB;MACAA,GAAG,CAACa,cAAc,GAAGb,GAAG,CAACG,SAAS,CAACvB,MAAM;IAC3C,CAAC,MAAM;MACL;MACAoB,GAAG,CAACa,cAAc,GAAG,CAAC;MACtBb,GAAG,CAACG,SAAS,GAAG,CAAE,CAAE,CAAC,EAAIH,GAAG,CAACO,WAAW,GAAG,CAAC,EAAI,CAAC,EAAE,CAAC,CAAE,CAAE;IAC1D;IAEA,IAAKvC,QAAQ,CAACkB,GAAG,EAAG;MAClB;MACAwB,EAAE,GAAG,IAAI;MACT,KAAM,IAAI7B,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGlC,WAAW,CAACiC,MAAM,EAAEC,CAAC,IAAI,CAAC,EAAG;QAChDlC,WAAW,CAAEkC,CAAC,CAAE,CAAC8B,cAAc,CAAED,EAAG,CAAC;QACrCA,EAAE,GAAG/D,WAAW,CAAEkC,CAAC,CAAE,CAAC+B,SAAS,CAAEH,eAAe,EAAE7D,eAAe,CAAEiC,CAAC,CAAG,CAAC;MAC1E;MACA;MACA;MACAmB,GAAG,CAACc,QAAQ,GAAGJ,EAAE;IACnB,CAAC,MAAM;MACLV,GAAG,CAACc,QAAQ,GAAG,EAAE;IACnB;IAEA,IAAK9C,QAAQ,CAACoB,QAAQ,EAAG;MACvB;MACAsB,EAAE,GAAG,IAAI;MACTA,EAAE,GAAG1D,WAAW,CAAC4D,SAAS,CAAEH,eAAgB,CAAC;MAC7CxD,SAAS,CAAEyD,EAAE,EAAEV,GAAG,EAAEtE,SAAS,EAAEE,MAAO,CAAC;IACzC;IAEA,IAAKoC,QAAQ,CAAC+C,SAAS,EAAG;MACxB;MACAL,EAAE,GAAG,IAAI;MACTA,EAAE,GAAGxD,UAAU,CAAC0D,SAAS,CAAEH,eAAgB,CAAC;MAC5CtD,QAAQ,CAAEuD,EAAE,EAAEV,GAAG,EAAEtE,SAAS,EAAEG,MAAO,CAAC;IACxC;IAEA,IAAKmC,QAAQ,CAACwB,GAAG,EAAG;MAClB;MACA,MAAMwB,OAAO,GAAGxF,4BAA4B,CAAEwE,GAAI,CAAC;MACnDU,EAAE,GAAG,IAAI;MACT,KAAM,IAAI7B,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGzB,WAAW,CAACwB,MAAM,EAAEC,CAAC,IAAI,CAAC,EAAG;QAChD6B,EAAE,GAAGtD,WAAW,CAAEyB,CAAC,CAAE,CAAC+B,SAAS,CAAEI,OAAO,EAAE3D,eAAe,CAAE,CAAC,CAAE,EAAE2C,GAAG,CAACE,MAAO,CAAC;QAC5E3C,UAAU,CAAEmD,EAAE,EAAEjE,KAAK,EAAEuE,OAAO,EAAEP,eAAgB,CAAC;MACnD;MACAnD,SAAS,CAAE0C,GAAG,EAAEgB,OAAO,EAAEjF,MAAM,EAAEL,SAAS,CAACuF,UAAW,CAAC;IACzD;IAEA,IAAKjD,QAAQ,CAACkD,GAAG,EAAG;MAClB;MACAR,EAAE,GAAG,IAAI;MACT,IAAKlD,WAAW,KAAK2D,SAAS,IAAIzD,YAAY,GAAG,CAAC,EAAG;QACnDE,SAAS,CAACoC,GAAG,GAAGA,GAAG;QACnBpC,SAAS,CAACW,QAAQ,GAAGZ,WAAW;QAChCC,SAAS,CAAClC,SAAS,GAAGA,SAAS;QAC/B,IAAKkC,SAAS,CAACwD,SAAS,EAAG5D,WAAW,CAACmD,cAAc,CAAEX,GAAG,CAACc,QAAS,CAAC;QACrEJ,EAAE,GAAGlD,WAAW,CAACoD,SAAS,CAAEH,eAAe,EAAEhD,cAAc,EAAEG,SAAU,CAAC;MAC1E;MACA;MACA;MACAoC,GAAG,CAACqB,cAAc,GAAGX,EAAE,IAAI,EAAE;IAC/B,CAAC,MAAMV,GAAG,CAACqB,cAAc,GAAG,EAAE;;IAG9B;IACA;IACA;IACA;;IAEA;IACA,IAAIC,GAAG,GAAGtG,GAAG,CAAEgF,GAAG,EAAE/D,QAAQ,CAACsF,MAAO,CAAC,CAAC,CAAC;;IAEvC;IACAjB,cAAc,CAACkB,KAAK,CAAC,CAAC;IACtB,OAAOF,GAAG;EACZ,CAAC,CAAC,CAAC;;EAEH,IAAIG,mBAAmB,GAAG,SAAAA,CAAWC,QAAQ,EAAEC,MAAM,EAAG;IACtD;IACA,IAAK7F,MAAM,CAAC8F,OAAO,CAAEF,QAAS,CAAC,EAAG;MAChCA,QAAQ,CAACG,OAAO,CAAIC,EAAE,IAAM;QAC1B,IAAKhG,MAAM,CAACiG,QAAQ,CAAED,EAAG,CAAC,EAAG;UAC3B;UACA,IAAO,OAAOA,EAAE,CAACE,IAAI,KAAK,QAAQ,IAAQF,EAAE,CAACE,IAAI,KAAK,EAAI,EAAG;YAC3D,MAAMjC,KAAK,CAAG,qDAAoD+B,EAAE,CAACE,IAAK,SAAQC,IAAI,CAACC,SAAS,CAAEJ,EAAE,EAAE,IAAI,EAAE,CAAE,CAAE,EAAE,CAAC;UACrH,CAAC,MAAM,IAAKhG,MAAM,CAAC8F,OAAO,CAAEE,EAAE,CAACK,QAAS,CAAC,EAAG;YAC1C,KAAM,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,EAAE,CAACK,QAAQ,CAACvD,MAAM,EAAEwD,CAAC,IAAI,CAAC,EAAG;cAChD,MAAMC,CAAC,GAAGP,EAAE,CAACK,QAAQ,CAAEC,CAAC,CAAE;cAC1B;cACA,IAAO,OAAOC,CAAC,KAAK,QAAQ,IAAQA,CAAC,KAAK,EAAI,EAAG;gBAC/C,MAAMtC,KAAK,CAAG,6DAA4DsC,CAAE,SAAQJ,IAAI,CAACC,SAAS,CAAEJ,EAAE,EAAE,IAAI,EAAE,CAAE,CAAE,EAAE,CAAC;cACvH;YACF,CAAC,CAAC;UACJ,CAAC,MAAM;YACL;YACA,MAAM/B,KAAK,CAAG,yDAAwD,OAAO+B,EAAE,CAACK,QAAS,SAAQF,IAAI,CAACC,SAAS,CAAEJ,EAAE,EAAE,IAAI,EAAE,CAAE,CAAE,EAAE,CAAC;UACpI;UACA;UACA;UACA,IAAOA,EAAE,CAACQ,IAAI,KAAKnB,SAAS,KACpB,CAACrF,MAAM,CAACyG,cAAc,CAAET,EAAE,CAACQ,IAAK,CAAC,IACjCR,EAAE,CAACQ,IAAI,CAAC1D,MAAM,KAAK,CAAG,IACtBkD,EAAE,CAACQ,IAAI,CAAC1D,MAAM,KAAK,CAAC,IAAIkD,EAAE,CAACQ,IAAI,CAAE,CAAC,CAAE,GAAGR,EAAE,CAACQ,IAAI,CAAE,CAAC,CAAI,CAAE,EAAG;YAChE,MAAMvC,KAAK,CAAG,uFAAsFkC,IAAI,CAACC,SAAS,CAAEJ,EAAE,CAACQ,IAAI,EAAE,IAAI,EAAE,CAAE,CAAE,EAAE,CAAC;UAC5I;QACF,CAAC,MAAM;UACL;UACA,MAAMvC,KAAK,CAAG,gEAA+D,OAAO+B,EAAG,SAAQG,IAAI,CAACC,SAAS,CAAEJ,EAAE,EAAE,IAAI,EAAE,CAAE,CAAE,EAAE,CAAC;QAClI;MACF,CAAE,CAAC;IACL,CAAC,MAAM;MACL;MACA,MAAM/B,KAAK,CAAG,yDAAwD,OAAO2B,QAAS,IAAI,CAAC;IAC7F;;IAEA;IACA9D,SAAS,GAAK+D,MAAM,KAAKR,SAAS,IAAIQ,MAAM,KAAK,IAAI,GAAKtF,MAAM,CAACC,MAAM,CAAE,IAAK,CAAC,GAAG2F,IAAI,CAACO,KAAK,CAAEP,IAAI,CAACC,SAAS,CAAEP,MAAO,CAAE,CAAC;IACxH,IAAK,CAAC7F,MAAM,CAACiG,QAAQ,CAAEnE,SAAU,CAAC,EAAG;MACnC,MAAMmC,KAAK,CAAG,wDAAuD,OAAOnC,SAAU,IAAI,CAAC;IAC7F;IACAA,SAAS,CAAC6E,UAAU,GAAG,CAAC,CAAC7E,SAAS,CAAC6E,UAAU;IAC7C7E,SAAS,CAAC8E,MAAM,GAAK9E,SAAS,CAAC8E,MAAM,KAAKvB,SAAS,GAAK,IAAI,GAAG,CAAC,CAACvD,SAAS,CAAC8E,MAAM;IACjF9E,SAAS,CAACwD,SAAS,GAAKxD,SAAS,CAACwD,SAAS,KAAKD,SAAS,GAAK,IAAI,GAAG,CAAC,CAACvD,SAAS,CAACwD,SAAS;;IAG1F;IACAvD,QAAQ,GAAGpC,QAAQ,CAAEqC,YAAY,EAAErB,KAAK,EAAEC,QAAQ,EAAEkB,SAAS,CAAC6E,UAAW,CAAC,CAAC,CAAC;;IAE5EjF,WAAW,GAAG,IAAI;IAClBE,YAAY,GAAG,CAAC;IAChBF,WAAW,GAAG7B,GAAG,CAAC,CAAC;IACnB,MAAMgH,QAAQ,GAAG9E,QAAQ,CAAC+E,GAAG,CAAElB,QAAS,CAAC;IACzC/D,WAAW,GAAGgF,QAAQ,CAACpE,QAAQ;IAC/Bb,YAAY,GAAGF,WAAW,CAACqF,KAAK,CAAEF,QAAQ,CAACjB,QAAS,CAAC;IACrD;IACA,OAAOhE,YAAY;EACrB,CAAC,CAAC,CAAC;;EAEH,IAAK5B,MAAM,CAACiG,QAAQ,CAAE9F,QAAS,CAAC,EAAG;IACjC,IAAK,OAAOA,QAAQ,CAACkC,IAAI,KAAK,UAAU,EAAG;MACzC,MAAM4B,KAAK,CAAE,+BAAgC,CAAC;IAChD;EACF,CAAC,MAAM;IACL,MAAMA,KAAK,CAAE,+BAAgC,CAAC;EAChD;;EAEA;EACA;EACAhC,gBAAgB,CAACU,GAAG,GAAG,OAAOxC,QAAQ,CAACwC,GAAG,KAAK,UAAU;EACzDV,gBAAgB,CAACqB,QAAQ,GAAG,OAAOnD,QAAQ,CAACmD,QAAQ,KAAK,UAAU;EACnErB,gBAAgB,CAACgD,SAAS,GAAG,OAAO9E,QAAQ,CAACqD,EAAE,KAAK,UAAU;EAC9DvB,gBAAgB,CAACyB,GAAG,GAAG,OAAOvD,QAAQ,CAACuD,GAAG,KAAK,UAAU;EACzDzB,gBAAgB,CAACmB,GAAG,GAAG,OAAOjD,QAAQ,CAACiD,GAAG,KAAK,UAAU;EACzDnB,gBAAgB,CAACmD,GAAG,GAAG,OAAOjF,QAAQ,CAAC2D,OAAO,KAAK,UAAU;EAE7D,IAAKzD,cAAc,KAAK,IAAI,EAAG;IAC7B,IAAK,CAACL,MAAM,CAACiG,QAAQ,CAAE5F,cAAe,CAAC,EACrC,MAAM4D,KAAK,CAAG,yEAAwE,OAAO5D,cAAe,IAAI,CAAC;IAEnH,IAAI2G,SAAS,GAAG,CAAC;IACjB,MAAMC,cAAc,GAAG1G,MAAM,CAACC,MAAM,CAAE,IAAK,CAAC;IAC5CyG,cAAc,CAACC,SAAS,GAAG,IAAI;IAC/BD,cAAc,CAACE,WAAW,GAAG,IAAI;IACjCF,cAAc,CAACG,SAAS,GAAG,IAAI;IAC/BH,cAAc,CAACI,UAAU,GAAG,IAAI;IAChCJ,cAAc,CAACK,SAAS,GAAG,IAAI;IAC/BL,cAAc,CAACM,IAAI,GAAG,IAAI;IAC1BN,cAAc,CAACO,KAAK,GAAG,IAAI;IAC3BP,cAAc,CAACQ,OAAO,GAAG,IAAI;IAC7B,KAAM,MAAMC,GAAG,IAAIrH,cAAc,EAAG;MAAE;MACpC2G,SAAS,IAAI,CAAC;MACd,IAAK,CAACC,cAAc,CAAES,GAAG,CAAE,EACzB,MAAMzD,KAAK,CAAE,wCAAyC,CAAC;IAC3D;IAEA,IAAK+C,SAAS,KAAK,CAAC,EAAG,MAAM/C,KAAK,CAAE,qCAAsC,CAAC;EAC7E;EAEA,MAAM0D,QAAQ,GAAKvH,IAAI,KAAK,IAAI,IAAIA,IAAI,KAAKiF,SAAS,GAAK9E,MAAM,CAACqH,IAAI,CAAE3F,gBAAiB,CAAC,GAAG7B,IAAI;EACjG,IAAKJ,MAAM,CAAC8F,OAAO,CAAE6B,QAAS,CAAC,EAAG;IAChCA,QAAQ,CAAC5B,OAAO,CAAI8B,EAAE,IAAM;MAC1B,IAAK,CAAC5F,gBAAgB,CAAE4F,EAAE,CAAE,EAAG,MAAM5D,KAAK,CAAG,sCAAqC4D,EAAG,UAAU,CAAC;MAChG3F,QAAQ,CAAE2F,EAAE,CAAE,GAAG,IAAI;MACrB1F,gBAAgB,GAAG,KAAK;IAC1B,CAAE,CAAC;EACL,CAAC,MAAM,MAAM8B,KAAK,CAAG,gEAA+D,OAAO7D,IAAK,IAAI,CAAC;;EAErG;EACAgC,IAAI,CAAC,CAAC;EACN;EACA;EACA;EACA9B,OAAO,CAACyD,OAAO,GAAGA,OAAO;EACzBzD,OAAO,CAACqF,mBAAmB,GAAGA,mBAAmB;EACjD;EACArF,OAAO,CAACwH,GAAG,GAAGvI,UAAU;EACxBe,OAAO,CAACyH,EAAE,GAAGvI,SAAS;EACtB;EACAc,OAAO,CAAC0H,QAAQ,GAAG,UAAWC,IAAI,EAAEC,IAAI,GAAG,IAAI,EAAI;IACjD,IAAK,CAAC7H,cAAc,EACpB,MAAM4D,KAAK,CAAE,qGAAsG,CAAC;IAEpH,MAAMwD,OAAO,GAAGpH,cAAc,CAACoH,OAAO;IACtC,MAAMH,SAAS,GAAGjH,cAAc,CAACiH,SAAS;IAC1C,MAAMa,SAAS,GAAG9H,cAAc,CAAC8G,WAAW,GAAG,CAAC;IAEhD,IAAK,OAAOc,IAAI,KAAK,QAAQ,EAAG;MAC9B,MAAMhE,KAAK,CAAE,6CAA8C,CAAC;IAC9D;IAEA,MAAMmE,EAAE,GAAGX,OAAO,CAAEQ,IAAI,CAACI,WAAW,CAAC,CAAC,CAAE;IACxC,IAAKD,EAAE,KAAK/C,SAAS,EAAG;MACtB;MACA,OAAS6C,IAAI,GAAKZ,SAAS,CAACgB,KAAK,CAAE,CAAC,EAAEH,SAAU,CAAC,GAAGb,SAAS,CAACgB,KAAK,CAAC,CAAC;IACvE;IACA,OAASJ,IAAI,GAAKE,EAAE,CAACE,KAAK,CAAE,CAAC,EAAEH,SAAU,CAAC,GAAGC,EAAE,CAACE,KAAK,CAAC,CAAC;EACzD,CAAC,CAAC,CAAC;;EAEH,OAAOhI,OAAO;AAChB,CAAC,CAAC,CAAC;;AAEHiI,MAAM,CAACC,OAAO,GAAGtI,GAAG","ignoreList":[]},"metadata":{},"sourceType":"script","externalDependencies":[]}